{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewfagence/Google-Gemma-XAI/blob/main/Study_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nJ166H5n2MY"
      },
      "source": [
        "# **Install Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e6vIbIGcg_3"
      },
      "source": [
        "Install dependencies via the '!pip install' command, which allows the Colab notebook to access the required libraries for downloading the Google Gemma LLM models, XAI methods and tools, benchmarking tools etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BppWos4tcgc9"
      },
      "outputs": [],
      "source": [
        "#Install Dependencies\n",
        "#Install condacolab, setup the conda notebook details, and install and upgrade pip.\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda create --name tf\n",
        "!conda activate tf\n",
        "!pip install --upgrade pip\n",
        "\n",
        "#Libraries which the Google Gemma LLM models are dependant on for importing and using\n",
        "!pip install -q keras-nlp\n",
        "!pip install -q transformers\n",
        "!pip install -q tensorflow\n",
        "!pip install -q accelerate\n",
        "!pip install -q torch\n",
        "!pip install -q torchinfo\n",
        "\n",
        "#Library for benchmarking tools to assess the selected Google Gemma LLM models\n",
        "!pip install -q deepeval\n",
        "\n",
        "#Libraries for importing and using XAI tools and methods\n",
        "!pip install -q lime\n",
        "!pip install -q bertviz\n",
        "!pip install -q datasets\n",
        "\n",
        "#Libraries for improving the selected Google Gemma LLM models\n",
        "!pip install -q peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bLb_-iXefnD"
      },
      "source": [
        "# **Set Environment Variables and Setup Backend Configurations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO1GzhcCMQpQ"
      },
      "source": [
        "Set environment variables and setup backend configurations which allows the necessary libraries required to be imported into the colab notebook, as well as the runtime instance having the correct settings/configurations. They also allow for the access of Google Gemma models on various hosting platforms such as Kaggle and Hugging Face, using their API with the various secrets attached to this colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh4FILOsC1p8"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import torch\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model, model_to_dot\n",
        "from datasets import load_dataset\n",
        "from torchinfo import summary\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from google.colab import drive\n",
        "\n",
        "#Setup the runtime instance with the correct settings/configurations\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # Or \"jax\" or \"torch\".\n",
        "\n",
        "#Mount Google Drive so figures and diagrams can be saved to a Google Drive Cloud location (For documentation purposes)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Knxwt1fiFvx"
      },
      "source": [
        "# **Import Selected Google Gemma LLM Models using Keras and the Kaggle API, and Print Out Model Architectures**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Gemma 2B LLM Model"
      ],
      "metadata": {
        "id": "TNDvQJURaT6k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h6Ki0haLkbK"
      },
      "source": [
        "Import a Google Gemma 2B LLM model using the Kaggle API, and print out a summary of its architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQiPDKcqLpYg"
      },
      "outputs": [],
      "source": [
        "gemma_lm2b = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\") #Import the \"gemma_2b_en\" model variation being hosted by Keras. Source:https://www.kaggle.com/models/google/gemma/keras/gemma_2b_en\n",
        "gemma_lm2b.summary() #Display a summary of the Google Gemma 2B LLM model.\n",
        "plot_model(gemma_lm2b, to_file='/content/drive/My Drive/Gemma_2b_eng_modelplot.png', show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True, show_trainable=True) #Print out the architecture of the Google Gemma 2B LLM model, and save it to the specified Google Drive location."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Gemma 2B Instruction-Tuned LLM Model"
      ],
      "metadata": {
        "id": "6FtTi-K3afzL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgfCM_u2tVJ-"
      },
      "source": [
        "Create a Google Gemma 2B Instruction-Tuned LLM model and print out a summary of its architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V2dx37wtlC5"
      },
      "outputs": [],
      "source": [
        "gemma_lm_it2b = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\") #Import the \"gemma_instruct_2b_en\" model variation being hosted by Keras. Source:https://www.kaggle.com/models/google/gemma/keras/gemma_instruct_2b_en\n",
        "gemma_lm_it2b.summary() #Display a summary of the Google Gemma 2B Instruction-Tuned LLM model.\n",
        "plot_model(gemma_lm_it2b, to_file='/content/drive/My Drive/Gemma_2b_instruct_eng_modelplot.png', show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True, show_trainable=True) #Print out the architecture of the Google Gemma 2B Instruction-Tuned LLM model, and save it to the specified Google Drive location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXy3nl-ZZ4-F"
      },
      "source": [
        "# **Gather Benchmark Results/Performance Metrics of the Selected Google Gemma LLM Models Before XAI Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tuUBpQdq1og"
      },
      "source": [
        "## Setup the Google Gemma LLM models to be evaluated, this step allows the selected models to function with the DeepEval library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek9N3C-nq6Zb"
      },
      "outputs": [],
      "source": [
        "#Setup the Google Gemma LLM models to be evaluated for benchmarking. Source:https://docs.confident-ai.com/docs/benchmarks-introduction#benchmarking-your-llm\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "\n",
        "class Gemma2BEN(DeepEvalBaseLLM):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def load_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        model = self.load_model()\n",
        "\n",
        "        device = \"cuda\" # Load the device onto a CUDA GPU for faster processing\n",
        "\n",
        "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "        model.to(device)\n",
        "\n",
        "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
        "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        return self.generate(prompt)\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return \"Gemma 2B EN\"\n",
        "\n",
        "#Create the model and tokenizer for the 'gemma_2b_en' model.\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
        "\n",
        "#Create the model and tokenizer for the 'gemma_instruct_2b_en' model\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "#Create the models using the deepeval class and defined function\n",
        "#gemma_2B_EN = Gemma2BEN(model=model, tokenizer=tokenizer)\n",
        "gemma_2B_EN_IT = Gemma2BEN(model=model2, tokenizer=tokenizer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coisJ2sc60g5"
      },
      "source": [
        "## Import the necessary libraries from the deepeval package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIFMuxvWJLZQ"
      },
      "outputs": [],
      "source": [
        "#Import the necessary benchmark tools and tasks from the deepeval library. Source:https://docs.confident-ai.com/docs/getting-started\n",
        "from deepeval.benchmarks import MMLU, HellaSwag, BigBenchHard, TruthfulQA, GSM8K #Import the benchmark type\n",
        "from deepeval.benchmarks.tasks import MMLUTask, HellaSwagTask, BigBenchHardTask, TruthfulQATask #Import the tasks for each benchmark type\n",
        "from deepeval.benchmarks.modes import TruthfulQAMode #Import the QAmode to be used on TruthfulQA benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GSM8K Benchmark"
      ],
      "metadata": {
        "id": "vA0QEW_Nsv_Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg2OlYRaPMTY"
      },
      "source": [
        "Setup the GSM8K benchmark to be performed on the Google Gemma LLM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xXhJfqbPgLQ"
      },
      "outputs": [],
      "source": [
        "#Define benchmark with specific tasks and shots for GSM8K. Source:https://docs.confident-ai.com/docs/benchmarks-gsm8k\n",
        "#To be used on the 'gemma_2b_en' model.\n",
        "benchmarkGSM8K = GSM8K(\n",
        "    n_problems=1319, #1319 is all problems available for the GSM8K benchmark.\n",
        "    n_shots=1, #Go through all problems only once (1-shot learning).\n",
        "    enable_cot=False #Disable Chain-of-Thoughts (CoT) prompting.\n",
        ")\n",
        "\n",
        "#To be used on the 'gemma_instruct_2b_en' model.\n",
        "benchmark2GSM8K = GSM8K(\n",
        "    n_problems=1319, #1319 is all problems available for the GSM8K benchmark.\n",
        "    n_shots=1, #Go through all problems only once (1-shot learning).\n",
        "    enable_cot=False #Disable Chain-of-Thoughts (CoT) prompting.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLx6dRA7aWdq"
      },
      "source": [
        "Perform the GSM8K benchmark evaluation on the 'gemma_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfwagFLDa_E4"
      },
      "outputs": [],
      "source": [
        "#Perform the GSM8K benchmark on the 'gemma_2b_en' Google Gemma LLM model and print the results\n",
        "benchmarkGSM8K.evaluate(model=gemma_2B_EN)\n",
        "print(benchmarkGSM8K.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM8OVc4DbZya"
      },
      "source": [
        "Perform the GSM8K benchmark evaluation on the 'gemma_instruct_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyWbId5xbsHw"
      },
      "outputs": [],
      "source": [
        "#Perform the GSM8K benchmark on the 'gemma_instruct_2b_en' Google Gemma LLM model and print the results\n",
        "benchmark2GSM8K.evaluate(model=gemma_2B_EN_IT)\n",
        "print(benchmark2GSM8K.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TruthfulQA Benchmark"
      ],
      "metadata": {
        "id": "YctSJr_KwIms"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ofCqQnFHQVT"
      },
      "source": [
        "Setup the TruthfulQA benchmark to be performed on the Google Gemma LLM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJQOYfA-HVDZ"
      },
      "outputs": [],
      "source": [
        "# Define benchmark with specific tasks and modes for TruthfulQA. Source:https://docs.confident-ai.com/docs/benchmarks-truthful-qa\n",
        "#To be used on the 'gemma_2b_en' model.\n",
        "benchmarkTruthfulQA = TruthfulQA(\n",
        "    mode=TruthfulQAMode.MC2 #Perform all 817 tasks under MC2 benchmarking mode\n",
        ")\n",
        "\n",
        "#To be used on the 'gemma_instruct_2b_en' model.\n",
        "benchmark2TruthfulQA = TruthfulQA(\n",
        "    mode=TruthfulQAMode.MC2 #Perform all 817 tasks under MC2 benchmarking mode\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L8gipRhKEQx"
      },
      "source": [
        "Perform the TruthfulQA benchmark evaluation on the 'gemma_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X41JpR_7g_lN"
      },
      "outputs": [],
      "source": [
        "#Perform the HumanEval benchmark on the 'gemma_2b_en' Google Gemma LLM model and print the results\n",
        "benchmarkTruthfulQA.evaluate(model=gemma_2B_EN)\n",
        "print(benchmarkTruthfulQA.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skiKh2ClhLpU"
      },
      "source": [
        "Perform the TruthfulQA benchmark evaluation on the 'gemma_instruct_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZortI5XkhVaE"
      },
      "outputs": [],
      "source": [
        "#Perform the HumanEval benchmark on the 'gemma_instruct_2b_en' Google Gemma LLM model and print the results\n",
        "benchmark2TruthfulQA.evaluate(model=gemma_2B_EN_IT)\n",
        "print(benchmark2TruthfulQA.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigBenchHard (BBH) Benchmark"
      ],
      "metadata": {
        "id": "jfA9SNsL2bQ_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtxlnOwWxFMJ"
      },
      "source": [
        "Setup the BigBenchHard (BBH) benchmark to be performed on the Google Gemma LLM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjNhbZ7exJh0"
      },
      "outputs": [],
      "source": [
        "# Define benchmark with specific tasks and shots for BigBenchHard (BBH). Source:https://docs.confident-ai.com/docs/benchmarks-big-bench-hard\n",
        "#To be used on the 'gemma_2b_en' model.\n",
        "benchmarkBBH = BigBenchHard(\n",
        "    tasks=[BigBenchHardTask.BOOLEAN_EXPRESSIONS, BigBenchHardTask.CAUSAL_JUDGEMENT, BigBenchHardTask.DATE_UNDERSTANDING, BigBenchHardTask.DISAMBIGUATION_QA, BigBenchHardTask.FORMAL_FALLACIES], #Define the tasks to perform for the BBH benchmark on the 'google_2b_en' model.\n",
        "    n_shots=1, #Go through each question once (1-shot learning).\n",
        "    enable_cot=False #Disable Chain-of-Thoughts (CoT)\n",
        ")\n",
        "\n",
        "#To be used on the 'gemma_instruct_2b_en' model.\n",
        "benchmark2BBH = BigBenchHard(\n",
        "    tasks=[BigBenchHardTask.BOOLEAN_EXPRESSIONS, BigBenchHardTask.CAUSAL_JUDGEMENT, BigBenchHardTask.DATE_UNDERSTANDING, BigBenchHardTask.DISAMBIGUATION_QA, BigBenchHardTask.FORMAL_FALLACIES], #Define the tasks to perform for the BBH benchmark on the 'google_instruct_2b_en' model.\n",
        "    n_shots=1, #Go through each question once (1-shot learning).\n",
        "    enable_cot=False #Disable Chain-of-Thoughts (CoT)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW8ltU7AxVZ8"
      },
      "source": [
        "Perform the BBH benchmark evaluation on the 'gemma_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em16pn3YxXwW"
      },
      "outputs": [],
      "source": [
        "#Perform the BBH benchmark on the 'gemma_2b_en' Google Gemma LLM model and print the results\n",
        "benchmarkBBH.evaluate(model=gemma_2B_EN)\n",
        "print(benchmarkBBH.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmgwP8nVxk6A"
      },
      "source": [
        "Perform the BBH benchmark evaluation on the 'gemma_instruct_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEz1BEuoxqVu"
      },
      "outputs": [],
      "source": [
        "#Perform the BBH benchmark on the 'gemma_instruct_2b_en' Google Gemma LLM model and print the results\n",
        "benchmark2BBH.evaluate(model=gemma_2B_EN_IT)\n",
        "print(benchmark2BBH.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HellaSwag Benchmark"
      ],
      "metadata": {
        "id": "n-Y7dtEW3ntx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_oWSDDQOBHm"
      },
      "source": [
        "Setup the HellaSwag benchmark to be performed on the Google Gemma LLM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQld_TO_OIwC"
      },
      "outputs": [],
      "source": [
        "# Define benchmark with specific tasks and shots for HellaSwag. Source:https://docs.confident-ai.com/docs/benchmarks-hellaswag\n",
        "#To be used on the 'gemma_2b_en' model.\n",
        "benchmarkHellaSwag = HellaSwag(\n",
        "    tasks=[HellaSwagTask.TRIMMING_BRANCHES_OR_HEDGES, HellaSwagTask.WASHING_HANDS, HellaSwagTask.PLAYING_POOL, HellaSwagTask.ZUMBA, HellaSwagTask.CRICKET, HellaSwagTask.BATON_TWIRLING, HellaSwagTask.PHILOSOPHY_AND_RELIGION, HellaSwagTask.GROOMING_DOG, HellaSwagTask.FIXING_THE_ROOF, HellaSwagTask.FIXING_BICYCLE], #Define the tasks to perform for the HellaSwag benchmark on the 'google_2b_en' model.\n",
        "    n_shots=1 #Go through each question once (1-shot learning).\n",
        ")\n",
        "\n",
        "#To be used on the 'gemma_instruct_2b_en' model.\n",
        "benchmark2HellaSwag = HellaSwag(\n",
        "    tasks=[HellaSwagTask.TRIMMING_BRANCHES_OR_HEDGES, HellaSwagTask.WASHING_HANDS, HellaSwagTask.PLAYING_POOL, HellaSwagTask.ZUMBA, HellaSwagTask.CRICKET, HellaSwagTask.BATON_TWIRLING, HellaSwagTask.PHILOSOPHY_AND_RELIGION, HellaSwagTask.GROOMING_DOG, HellaSwagTask.FIXING_THE_ROOF, HellaSwagTask.FIXING_BICYCLE], #Define the tasks to perform for the HellaSwag benchmark on the 'google_instruct_2b_en' model.\n",
        "    n_shots=1 #Go through each question once (1-shot learning).\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-aqYFQQjCv"
      },
      "source": [
        "Perform the HellaSwag benchmark evaluation on the 'gemma_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDj5UFHufyvH"
      },
      "outputs": [],
      "source": [
        "#Perform the HellaSwag benchmark on the 'gemma_2b_en' Google Gemma LLM model and print the results\n",
        "benchmarkHellaSwag.evaluate(model=gemma_2B_EN)\n",
        "print(benchmarkHellaSwag.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKz7x48NgAj6"
      },
      "source": [
        "Perform the HellaSwag benchmark evaluation on the 'gemma_instruct_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovPERAKtgIFa"
      },
      "outputs": [],
      "source": [
        "#Perform the HellaSwag benchmark on the 'gemma_instruct_2b_en' Google Gemma LLM model and print the results\n",
        "benchmark2HellaSwag.evaluate(model=gemma_2B_EN_IT)\n",
        "print(benchmark2HellaSwag.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MMLU Benchmark"
      ],
      "metadata": {
        "id": "AK_ifufnFHkt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8-LnchWltM_"
      },
      "source": [
        "Setup the MMLU benchmark to be performed on the Google Gemma LLM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icDdSiTQl3hJ"
      },
      "outputs": [],
      "source": [
        "# Define benchmark with specific tasks and shots for Massive Multitask Language Understanding (MMLU). Source:https://docs.confident-ai.com/docs/benchmarks-mmlu\n",
        "#To be used on the 'gemma_2b_en' model.\n",
        "benchmarkMMLU = MMLU(\n",
        "    tasks=[MMLUTask.BUSINESS_ETHICS, MMLUTask.HIGH_SCHOOL_PHYSICS, MMLUTask.HIGH_SCHOOL_WORLD_HISTORY, MMLUTask.HIGH_SCHOOL_MICROECONOMICS, MMLUTask.HIGH_SCHOOL_BIOLOGY, MMLUTask.PHILOSOPHY, MMLUTask.ANATOMY, MMLUTask.COLLEGE_CHEMISTRY, MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ELECTRICAL_ENGINEERING], #Define the tasks to perform for the MMLU benchmark on the 'google_2b_en' model.\n",
        "    n_shots=1 #Go through each question once (1-shot learning).\n",
        ")\n",
        "\n",
        "#To be used on the 'gemma_instruct_2b_en' model.\n",
        "benchmark2MMLU = MMLU(\n",
        "    tasks=[MMLUTask.BUSINESS_ETHICS, MMLUTask.HIGH_SCHOOL_PHYSICS, MMLUTask.HIGH_SCHOOL_WORLD_HISTORY, MMLUTask.HIGH_SCHOOL_MICROECONOMICS, MMLUTask.HIGH_SCHOOL_BIOLOGY, MMLUTask.PHILOSOPHY, MMLUTask.ANATOMY, MMLUTask.COLLEGE_CHEMISTRY, MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ELECTRICAL_ENGINEERING], #Define the tasks to perform for the MMLU benchmark on the 'google_instruct_2b_en' model.\n",
        "    n_shots=1 #Go through each question once (1-shot learning).\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDXS9lTay1QA"
      },
      "source": [
        "Perform the MMLU benchmark evaluation on the 'gemma_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjdG3lB3HnXl"
      },
      "outputs": [],
      "source": [
        "#Perform the MMLU benchmark evaluation on the 'gemma_2b_en' model, and print out the score.\n",
        "benchmarkMMLU.evaluate(model=gemma_2B_EN)\n",
        "print(benchmarkMMLU.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCVpRS9I1lfO"
      },
      "source": [
        "Perform the MMLU benchmark evaluation on the 'gemma_instruct_2b_en' model, and print out the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDz_DalA1tpI"
      },
      "outputs": [],
      "source": [
        "#Perform the MMLU benchmark evaluation on the 'gemma_instruct_2b_en' model, and print out the score.\n",
        "benchmark2MMLU.evaluate(model=gemma_2B_EN_IT)\n",
        "print(benchmark2MMLU.overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n--iL8NoUhJH"
      },
      "source": [
        "# **Apply XAI methods to the Selected Google Gemma Models for Pre-XAI Modification Explanations Using the Hugging Face API.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKWsX--qLlcZ"
      },
      "source": [
        "## Import selected Google Gemma models using the HuggingFace API, and create a list of 15 questions in various categories."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a list of 15 questions in different categories, the different XAI tools and methods will use these questions to evaluate the selected Google Gemma LLM models. The questions have been spread over 5 different categories and each question has a different level of complexity, this ensures the selected Google Gemma LLM models are evaluated thoroughly."
      ],
      "metadata": {
        "id": "NRfWWZCSkRZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7WEgsU_MImw"
      },
      "outputs": [],
      "source": [
        "#Create the model and tokenizer for the 'gemma_2b_en' model. Comment out when necessary to use/unuse the model (for resource reasons). To be used with the Bertviz attention-visualisation and HotpotQA XAI tools.\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", output_attentions=True, attn_implementation=\"eager\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=True)\n",
        "model.config.is_decoder = True\n",
        "\n",
        "#Create the model and tokenizer for the 'gemma_2b_en' model. Comment out when necessary to use/unuse the model (for resource reasons). To be used with the LIME XAI tool.\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(\"google/gemma-2b\", num_labels=2)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=True)\n",
        "\n",
        "#Create the model and tokenizer for the 'gemma_instruct_2b_en' model. Comment out when necessary to use/unuse the model (for resource reasons). To be used with the Bertviz attention-visualisation and HotpotQA XAI tools.\n",
        "#model2 = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", output_attentions=True, attn_implementation=\"eager\")\n",
        "#tokenizer2 = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", use_fast=True)\n",
        "#model2.config.is_decoder = True\n",
        "\n",
        "#Create the model and tokenizer for the 'gemma_instruct_2b_en' model. Comment out when necessary to use/unuse the model (for resource reasons). To be used with the LIME XAI tool.\n",
        "#model2 = AutoModelForSequenceClassification.from_pretrained(\"google/gemma-2b-it\", num_labels=2)\n",
        "#tokenizer2 = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", use_fast=True)\n",
        "\n",
        "#15 questions in total will be asked over general, maths, science, programming/code and paradoxical categories.\n",
        "questions = [] #Create a new list to hold the text questions\n",
        "#3 General Questions\n",
        "questions.append(\"How many minutes would it take me to walk from the Northumbria University campus located in Newcastle upon Tyne, to the city centre located in Newcastle upon Tyne?\") #Question 1\n",
        "questions.append(\"To date, who is the most decorated Olympian?\") #Question 2\n",
        "questions.append(\"Can you list the 'New 7 Wonders of the World'?\") #Question 3\n",
        "#3 Maths Questions\n",
        "questions.append(\"What is the value of 'x' in this equation: 18 + 8x = 30\") #Question 4\n",
        "questions.append(\"What is the limit of 'sin(x)/x' as x approaches 0?\") #Question 5\n",
        "questions.append(\"What is the value of 'Pi' to 30 decimal places?\") #Question 6\n",
        "#3 Science Questions\n",
        "questions.append(\"What happens to a star once it reaches the end of its life?\") #Question 7\n",
        "questions.append(\"What is the half-life of hydrogen-3 (tritium)?\") #Question 8\n",
        "questions.append(\"What is 'CRISPR' technology used for?\") #Question 9\n",
        "#3 Programming/Code Questions\n",
        "questions.append(\"Create a pseudocode method/function that checks to see if a number is either even or odd.\") #Question 10\n",
        "questions.append(\"What is the time complexity and space complexity of the merge sort algorithm?\") #Question 11\n",
        "questions.append(\"What are the key differences between the programming languages 'Python', 'C' and 'Assembly'?\") #Question 12\n",
        "#3 Paradoxical Questions\n",
        "questions.append(\"Is the sentence 'This sentence is false.' true or false?\") #Question 13\n",
        "questions.append(\"If there was a set of all sets, would that set contain itself?\") #Question 14\n",
        "questions.append(\"What happens when an unstoppable object collides with an immovable object?\") #Question 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH_I37GExlcG"
      },
      "source": [
        "## Apply General (Model-Agnostic) XAI methods to the Google Gemma 2B and Google Gemma 2B Instruction-Tuned models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eePBueab6E6e"
      },
      "source": [
        "### LIME Explanations for the Google Gemma 2B Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIME stands for \"**L**ocal **I**nterpretable **M**odel-agnostic **E**xplanations\" and this XAI tool can be used to give explanations to all kinds of machine learning models/AI systems, regardless of their complexity and purpose. The LIME XAI tool aims to give understanding to machine learning models/AI systems by interpreting their behaviour within a specific and particular instance, and using simple model structures to approximate values and behaviour.\n",
        "\n",
        "With textual models and data, LIME identifies which tokens contributes the most within an input to an LLM model by changing and removing different tokens in an input. The \"LimeTextExplaier\" library will be used, this library functions by applying an exponential kernel on cosine distance, and restricting explanations to words that are present in documents. The LIME XAI tool will be used on the Google Gemma 2B model with the list of 15 questions created for evaluation and investigation."
      ],
      "metadata": {
        "id": "R63ODyU0RI7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gCJGAjn6Jsw"
      },
      "outputs": [],
      "source": [
        "#LIME Explanations to be used on the Google Gemma 2B model. Source: https://lime-ml.readthedocs.io/en/latest/lime.html\n",
        "#Import the LIME \"Text Explainer\" library to use the LIME XAI tool on the Google Gemma 2B LLM model\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "#Define a function which predicts a tensorflow value for each token in an input being fed into the Google Gemma 2B model, to be used with the LIME text explainer.\n",
        "def gemma2b_model_predictions(question):\n",
        "  modelInputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True)\n",
        "  modelOutputs = model(**modelInputs)\n",
        "  modelLogits = modelOutputs.logits\n",
        "  modelProbabilities = torch.nn.functional.softmax(modelLogits, dim=-1).detach().numpy()\n",
        "  return modelProbabilities\n",
        "\n",
        "#With every question contained in the question array, use the LIME XAI tool/method and output the results\n",
        "LIMEexplainer = LimeTextExplainer(class_names=['Negative', 'Positive'])\n",
        "for question in questions:\n",
        "  explanation = LIMEexplainer.explain_instance(question, gemma2b_model_predictions, labels=[0, 1], num_samples=100)\n",
        "  explanation.show_in_notebook(text=question)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIME Explanations for the Google Gemma 2B Instruction-Tuned Model"
      ],
      "metadata": {
        "id": "htiifTcpC6Wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIME stands for \"**L**ocal **I**nterpretable **M**odel-agnostic **E**xplanations\" and this XAI tool can be used to give explanations to all kinds of machine learning models/AI systems, regardless of their complexity and purpose. The LIME XAI tool aims to give understanding to machine learning models/AI systems by interpreting their behaviour within a specific and particular instance, and using simple model structures to approximate values and behaviour.\n",
        "\n",
        "With textual models and data, LIME identifies which tokens contributes the most within an input to an LLM model by changing and removing different tokens in an input. The \"LimeTextExplaier\" library will be used, this library functions by applying an exponential kernel on cosine distance, and restricting explanations to words that are present in documents. The LIME XAI tool will be used on the Google Gemma 2B instruction-tuned model with the list of 15 questions created for evaluation and investigation."
      ],
      "metadata": {
        "id": "DdGtUGfOIVwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIME Explanations to be used on the Google Gemma 2B instruction-tuned model. Source: https://lime-ml.readthedocs.io/en/latest/lime.html\n",
        "#Import the necessary libraries to use the LIME XAI tool on the Google Gemma 2B instruction-tuned LLM model\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "#Define a function which predicts a tensorflow value for each token in an input being fed into the Google Gemma 2B instruction-tuned model, to be used with the LIME text explainer.\n",
        "def gemma2bit_model_predictions(question):\n",
        "  modelInputs2 = tokenizer2(question, return_tensors='pt', padding=True, truncation=True)\n",
        "  modelOutputs2 = model2(**modelInputs2)\n",
        "  modelLogits2 = modelOutputs2.logits\n",
        "  modelProbabilities2 = torch.nn.functional.softmax(modelLogits2, dim=-1).detach().numpy()\n",
        "  return modelProbabilities2\n",
        "\n",
        "#With every question contained in the question array, use the LIME XAI tool/method and output the results\n",
        "LIMEexplainer2 = LimeTextExplainer(class_names=['Negative', 'Positive'])\n",
        "for question in questions:\n",
        "  explanation2 = LIMEexplainer2.explain_instance(question, gemma2bit_model_predictions, labels=[0, 1], num_samples=100)\n",
        "  explanation2.show_in_notebook(text=question)"
      ],
      "metadata": {
        "id": "SPHdxITvDH6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJyrH9P-E_Fw"
      },
      "source": [
        "## Apply Model-Specific XAI methods to the Google Gemma 2B and Google Gemma 2B Instruction-Tuned Models (LLM/Deep Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGLKA-ClJmxG"
      },
      "source": [
        "### Attention-Visualization Explanations for the Google Gemma 2B Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BertViz attention-visualisation XAI tool is an open-source and interactive lens that allows users/developers to peek inside the workings of machine learning models and AI systems, specifically deep learning models and neural network models. The BertViz attention-visualisation XAI tool aims to give understanding to deep learning/neural network models by translating numerical data into diagrams which can be interpreted for investigation and analysis.\n",
        "\n",
        "The BertViz attention-visualisation XAI tool has 3 different flavours/views available to investigate and analyze deep learning/neural network models. There is the 'head view' which allows users/developers to view attention for 1 or more attention heads in the same layer of a model, the 'model view' which allows users/developers to view attention across all layers and heads within a model, and the 'neuron view' which allows users/developers to view attention for individual neurons within a model. The BertViz attention-visualisation XAI tool will be used on the Google Gemma 2B model with the list of 15 questions created, and with the 'model view' for evaluation and investigation."
      ],
      "metadata": {
        "id": "ltNl6xp782Hm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gC0i3Bljduv"
      },
      "outputs": [],
      "source": [
        "#Use the BertViz Attention-Visualisation library on the Google Gemma 2B model. Source: https://pypi.org/project/bertviz/\n",
        "# Import the \"model_view\" library from BertViz, to be used on the Google Gemma 2B model\n",
        "from bertviz import model_view\n",
        "\n",
        "# For each question in the list of questions, tokenize the question, feed it into the Google Gemma 2B model to obtain outputs, then apply the BertViz \"model_view\" XAI tool to obtain attention-visualization outputs\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    attention = outputs.attentions\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    model_view(attention, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention-Visualization Explanations for the Google Gemma 2B Instruction-Tuned Model"
      ],
      "metadata": {
        "id": "Tc2Vm0WJFQn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BertViz attention-visualisation XAI tool is an open-source and interactive lens that allows users/developers to peek inside the workings of machine learning models and AI systems, specifically deep learning models and neural network models. The BertViz attention-visualisation XAI tool aims to give understanding to deep learning/neural network models by translating numerical data into diagrams which can be interpreted for investigation and analysis.\n",
        "\n",
        "The BertViz attention-visualisation XAI tool has 3 different flavours/views available to investigate and analyze deep learning/neural network models. There is the 'head view' which allows users/developers to view attention for 1 or more attention heads in the same layer of a model, the 'model view' which allows users/developers to view attention across all layers and heads within a model, and the 'neuron view' which allows users/developers to view attention for individual neurons within a model. The BertViz attention-visualisation XAI tool will be used on the Google Gemma 2B instruction-tuned model with the list of 15 questions created, and with the 'model view' for evaluation and investigation."
      ],
      "metadata": {
        "id": "7aQHalZuCVRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the BertViz Attention-Visualisation library on the Google Gemma 2B instruction-tuned model. Source: https://pypi.org/project/bertviz/\n",
        "# Import the \"model_view\" library from BertViz, to be used on the Google Gemma 2B instruction-tuned model\n",
        "from bertviz import model_view\n",
        "\n",
        "# For each question in the list of questions, tokenize the question, feed it into the Google Gemma 2B instruction-tuned model to obtain outputs, then apply the BertViz \"model_view\" XAI tool to obtain attention-visualization outputs\n",
        "for question in questions:\n",
        "    inputs2 = tokenizer2(question, return_tensors='pt')\n",
        "    outputs2 = model2(**inputs2)\n",
        "    attention2 = outputs2.attentions\n",
        "    tokens2 = tokenizer2.convert_ids_to_tokens(inputs2['input_ids'][0])\n",
        "    model_view(attention2, tokens2)"
      ],
      "metadata": {
        "id": "DhjW4u7cFYJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply XAI methods to Data/Datasets being fed into the Google Gemma 2B and Google Gemma 2B Instruction-Tuned models"
      ],
      "metadata": {
        "id": "w0pWKyuSVl1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HotpotQA Explanations for the Google Gemma 2B Model"
      ],
      "metadata": {
        "id": "eJYxgSwcEyLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The HotpotQA XAI tool is a question-answering dataset, created by a collective team of NLP researchers at Carnegie Mellon University, Stanford University, and Université de Montréal. The HotpotQA XAI tool question-answering dataset is Wikipedia-based, with 113k question-answer pairs making up its entirety. These question-answer pairs are split up into various partitions, a 'train' partition which contains 90447 pairs, a 'validation' partition which contains 7405 pairs and a 'test' partition which also contains 7405 pairs.\n",
        "\n",
        "The HotpotQA XAI tool functions by using natural, multi-hop questions, with strong supervision for supporting facts via Wikipedia-based content to enable more explainable question answering systems. With the Google Gemma 2B model, the HotpotQA XAI tool is applied using the first 15 questions from the validation partition of the 'fullwiki' dataset. The generated answer from the Google Gemma 2B model is compared with the ground truth answer contained within the dataset, and an F1 score is calculated from comparing the generated answer with the ground truth answer with each question."
      ],
      "metadata": {
        "id": "rL53huTX1Fdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the HotpotQA library on the Google Gemma 2B model. Source: https://hotpotqa.github.io/?ref=the-batch-deeplearning-ai\n",
        "# Load the Wikipedia-based HotpotQA library using the 'fullwiki' option\n",
        "hpQADataset = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
        "\n",
        "# Use the 'validation' partition of the 'fullwiki' dataset and place it within its own variable\n",
        "validationDataset = hpQADataset['validation']\n",
        "\n",
        "# Select the first 15 questions within the validation samples dataset, to use HotpotQA techniques with the Gemma 2B model\n",
        "validationSamples = validationDataset.select(range(15))\n",
        "\n",
        "# For each question in the list of 15 selected validation question, print them out for display and inspection purposes\n",
        "for vs in validationSamples:\n",
        "  print(vs)\n",
        "\n",
        "# Define a function which tokenizes inputs and context strings from each validation sample, for the Google Gemma 2B model to process\n",
        "def tokenizeInput(question, context):\n",
        "  contextstr = \" \".join(context)\n",
        "  modelInputs = tokenizer.encode_plus(question, contextstr, return_tensors=\"pt\")\n",
        "  return modelInputs\n",
        "\n",
        "# Define a function which computes an F1 for each answer the Google Gemma 2B model, it compares an output prediction with a truthful prediction and calculates precision/recall, which can then be used to calculate an F1 score\n",
        "def computeF1(pred, true):\n",
        "  predTokens = pred.lower().split()\n",
        "  trueTokens = true.lower().split()\n",
        "\n",
        "  commonTokens = set(predTokens) & set(trueTokens)\n",
        "  if len(commonTokens) == 0:\n",
        "    return 0.0\n",
        "\n",
        "  precision = len(commonTokens) / len(predTokens)\n",
        "  recall = len(commonTokens) / len(trueTokens)\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "# For each question in the validation dataset, separate each element into their own variable and input the question into the Gemma 2B model. Compare the output prediction of the Google Gemma 2B model with the truthful prediction and output/print performance metrics.\n",
        "for i, sample in enumerate(validationSamples):\n",
        "  question = sample['question']\n",
        "  context = sample['context']\n",
        "  groundTruth = sample.get('answer')\n",
        "\n",
        "  if groundTruth is None:\n",
        "    print(f\"Sample {i+1} has no ground truth text. Skipping.\")\n",
        "    continue\n",
        "\n",
        "  tokenizedInput = tokenizeInput(question, context)\n",
        "  inputids = tokenizedInput[\"input_ids\"]\n",
        "  modelOutputs = model.generate(inputids, max_length=150)\n",
        "  modelAnswer = tokenizer.decode(modelOutputs[0], skip_special_tokens=True)\n",
        "\n",
        "  exactMatch = modelAnswer.strip().lower() == groundTruth.strip().lower()\n",
        "  F1Score = computeF1(modelAnswer, groundTruth)\n",
        "\n",
        "  print(f\"Sample {i+1}\")\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Generated Answer: {modelAnswer}\")\n",
        "  print(f\"Ground Truth: {groundTruth}\")\n",
        "  print(f\"Exact Match: {exactMatch}\")\n",
        "  print(f\"F1 Score: {F1Score}\")\n",
        "  print(\"----------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "1Nawsi_tF6a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HotpotQA Explanations for the Google Gemma 2B Instruction-Tuned Model"
      ],
      "metadata": {
        "id": "7FMPUdCRhKoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The HotpotQA XAI tool is a question-answering dataset, created by a collective team of NLP researchers at Carnegie Mellon University, Stanford University, and Université de Montréal. The HotpotQA XAI tool question-answering dataset is Wikipedia-based, with 113k question-answer pairs making up its entirety. These question-answer pairs are split up into various partitions, a 'train' partition which contains 90447 pairs, a 'validation' partition which contains 7405 pairs and a 'test' partition which also contains 7405 pairs.\n",
        "\n",
        "The HotpotQA XAI tool functions by using natural, multi-hop questions, with strong supervision for supporting facts via Wikipedia-based content to enable more explainable question answering systems. With the Google Gemma 2B instruction-tuned model, the HotpotQA XAI tool is applied using the first 15 questions from the validation partition of the 'fullwiki' dataset. The generated answer from the Google Gemma 2B instruction-tuned model is compared with the ground truth answer contained within the dataset, and an F1 score is calculated from comparing the generated answer with the ground truth answer with each question."
      ],
      "metadata": {
        "id": "fKCE76QI6Idb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the HotpotQA library on the Google Gemma 2B instruction-tuned model. Source: https://hotpotqa.github.io/?ref=the-batch-deeplearning-ai\n",
        "# Load the Wikipedia-based HotpotQA library using the 'fullwiki' option\n",
        "hpQADataset2 = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
        "\n",
        "# Use the 'validation' partition of the 'fullwiki' dataset and place it within its own variable\n",
        "validationDataset2 = hpQADataset2['validation']\n",
        "\n",
        "# Select the first 15 questions within the validation samples dataset, to use HotpotQA techniques with the Gemma 2B instruction-tuned model\n",
        "validationSamples2 = validationDataset2.select(range(15))\n",
        "\n",
        "# For each question in the list of 15 selected validation question, print them out for display and inspection purposes\n",
        "for vs in validationSamples2:\n",
        "  print(vs)\n",
        "\n",
        "# Define a function which tokenizes inputs and context strings from each validation sample, for the Google Gemma 2B instruction-tuned model to process\n",
        "def tokenizeInput(question, context):\n",
        "  contextstr = \" \".join(context)\n",
        "  modelInputs = tokenizer2.encode_plus(question, contextstr, return_tensors=\"pt\")\n",
        "  return modelInputs\n",
        "\n",
        "# Define a function which computes an F1 for each answer the Google Gemma 2B instruction-tuned model, it compares an output prediction with a truthful prediction and calculates precision/recall, which can then be used to calculate an F1 score\n",
        "def computeF1(pred, true):\n",
        "  predTokens = pred.lower().split()\n",
        "  trueTokens = true.lower().split()\n",
        "\n",
        "  commonTokens = set(predTokens) & set(trueTokens)\n",
        "  if len(commonTokens) == 0:\n",
        "    return 0.0\n",
        "\n",
        "  precision = len(commonTokens) / len(predTokens)\n",
        "  recall = len(commonTokens) / len(trueTokens)\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "# For each question in the validation dataset, separate each element into their own variable and input the question into the Gemma 2B instruction-tuned model. Compare the output prediction of the Google Gemma 2B model with the truthful prediction and output/print performance metrics.\n",
        "for i, sample in enumerate(validationSamples2):\n",
        "  question = sample['question']\n",
        "  context = sample['context']\n",
        "  groundTruth = sample.get('answer')\n",
        "\n",
        "  if groundTruth is None:\n",
        "    print(f\"Sample {i+1} has no ground truth text. Skipping.\")\n",
        "    continue\n",
        "\n",
        "  tokenizedInput2 = tokenizeInput(question, context)\n",
        "  inputids2 = tokenizedInput2[\"input_ids\"]\n",
        "  modelOutputs2 = model2.generate(inputids2, max_length=150)\n",
        "  modelAnswer2 = tokenizer2.decode(modelOutputs2[0], skip_special_tokens=True)\n",
        "\n",
        "  exactMatch = modelAnswer2.strip().lower() == groundTruth.strip().lower()\n",
        "  F1Score = computeF1(modelAnswer2, groundTruth)\n",
        "\n",
        "  print(f\"Sample {i+1}\")\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Generated Answer: {modelAnswer2}\")\n",
        "  print(f\"Ground Truth: {groundTruth}\")\n",
        "  print(f\"Exact Match: {exactMatch}\")\n",
        "  print(f\"F1 Score: {F1Score}\")\n",
        "  print(\"----------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "DUB9Gt9aiXMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implement Improvements on the Selected Google Gemma LLM Models with Guidance and Reasoning from XAI Outputs**"
      ],
      "metadata": {
        "id": "EENCxtpr72cE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Improvements on the Google Gemma 2B Model"
      ],
      "metadata": {
        "id": "cyxyYVzR-AvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the Google Gemma 2B Model"
      ],
      "metadata": {
        "id": "s53_tAtdDuku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model, tokenizer and configurations for the 'gemma_2b_en' model. To be used within improvement steps.\n",
        "modelConfig = AutoConfig.from_pretrained(\"google/gemma-2b\")\n",
        "model = AutoModel.from_pretrained(\"google/gemma-2b\", config=modelConfig)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=True)"
      ],
      "metadata": {
        "id": "btG7caZ2D5lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvement 3 - Modifying/Tweaking the Architecture of the Google Gemma 2B Model"
      ],
      "metadata": {
        "id": "lClmrRtkYtPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the configuration settings and model architecture of the Google Gemma 2B model"
      ],
      "metadata": {
        "id": "snIAI0uIVnpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print out the configuration settings and the architecture of the Google Gemma 2B model for inspection purposes.\n",
        "print(\"Model Configurations:\")\n",
        "print(modelConfig)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Model Architecture:\")\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "LBRbWl4KZDz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edit/Tweak the Google Gemma 2B model configuration JSON file with new configurations"
      ],
      "metadata": {
        "id": "ukEql5yvC3E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the configurations for the 'gemma_2b_en' model. To be used within improvement steps.\n",
        "modelConfig = AutoConfig.from_pretrained(\"google/gemma-2b\")\n",
        "\n",
        "# Edit/Tweak the 'gemma_2b_en' model configuration JSON file with new configurations, print the new model configurations, then set the model to use the newly updated configuration JSON file.\n",
        "modelConfig.attention_dropout = 0.1 #\n",
        "modelConfig.torch_dtype = \"float32\" #\n",
        "modelConfig.rms_norm_eps = 1e-05 #\n",
        "\n",
        "modelConfig.save_pretrained(\"/content/drive/My Drive/ConfigJSONGemma2B/\")\n",
        "\n",
        "modelConfigEdited = AutoConfig.from_pretrained(\"/content/drive/My Drive/ConfigJSONGemma2B/\")\n",
        "\n",
        "print(\"Edited/Tweaked Model Configurations:\")\n",
        "print(modelConfigEdited)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", config=modelConfigEdited).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=True)"
      ],
      "metadata": {
        "id": "bRHSr7GFFms3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvement 4 - Applying Fine-Tuning Methods/Techniques to the Google Gemma 2B Model"
      ],
      "metadata": {
        "id": "C_g9aBpia9P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and Setup the PEFT library to use LoRA (Low-Rank Adoption)  "
      ],
      "metadata": {
        "id": "FQ7Li0MS59pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import and setup the PEFT library in order to use the LoRA fine-tuning method during training processes. Source: https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
        "#Import the necessary libraries to use the PEFT fine-tuning tool.\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "#Setup the imported PEFT library for fine-tuning methods during training processes.\n",
        "loraConfigurations = LoraConfig(\n",
        "  task_type = TaskType.CAUSAL_LM,\n",
        "  r = 32,\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0.1,\n",
        "  target_modules = [\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "#Initialize a PEFT model using the Gemma 2B LLM model and the Lora Configuration variable.\n",
        "loraModel = get_peft_model(model, loraConfigurations).to(\"cuda\")"
      ],
      "metadata": {
        "id": "N7JnCR_AbxsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvements 1 and 2 - Applying Additional Training & Enable N-Shot Learning for the Google Gemma 2B Model"
      ],
      "metadata": {
        "id": "ntyTs6GxQA7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wikipedia Dataset"
      ],
      "metadata": {
        "id": "4b_tGOM_MJhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Google Gemma 2B model on the Wikipedia dataset. Source: https://huggingface.co/datasets/legacy-datasets/wikipedia\n",
        "# Retrieve and load a pre-processed version of the 'train' split of the Wikipedia dataset from HuggingFace.\n",
        "wikipediaDS = load_dataset(\"wikipedia\", \"20220301.en\", split='train', trust_remote_code=True)\n",
        "\n",
        "# Randomize the order of the entries within the Wikipedia dataset, and use a seed so it can be reproduced.\n",
        "wikipediaDS = wikipediaDS.shuffle(seed=42)\n",
        "\n",
        "# Select 50000 examples from the randomized Wikipedia Dataset (for training time purposes with available resources)\n",
        "wikipediaDS = wikipediaDS.select(range(50000))\n",
        "\n",
        "# Split the wikipedia dataset into train and validation sets (90% for train and 10% for validation) for the training processes.\n",
        "wikipediaDSSplit = wikipediaDS.train_test_split(test_size=0.1)\n",
        "wikipediaDSTrain = wikipediaDSSplit['train']\n",
        "wikipediaDSValid = wikipediaDSSplit['test']\n",
        "\n",
        "# Create a function which will tokenize the 'text' section of a wikipedia dataset entry.\n",
        "def tokenizeFunctionWP(inputText):\n",
        "  return tokenizer(inputText['text'], padding=True, truncation=True, max_length=350) # max_length implemented to save environment resources and stop resource errors.\n",
        "\n",
        "# Tokenize the entire text sections of all the wikipedia train dataset entries, and map this to its own variable to be called upon later.\n",
        "wikipediaDSTTokenized = wikipediaDSTrain.map(tokenizeFunctionWP, batched=True)\n",
        "\n",
        "# Tokenize the entire text sections of all the wikipedia validation dataset entries, and map this to its own variable to be called upon later.\n",
        "wikipediaDSVTokenized = wikipediaDSValid.map(tokenizeFunctionWP, batched=True)\n",
        "\n",
        "# Initialise the 'TrainingArguments' tool, and input specific arguments for how the model will be trained on the wikipedia dataset.\n",
        "trainingParamsWP = TrainingArguments(\n",
        "  output_dir = '/content/drive/My Drive/WikipediaTrainResults',\n",
        "  eval_strategy = 'epoch',\n",
        "  per_device_train_batch_size = 2,\n",
        "  gradient_accumulation_steps = 8,\n",
        "  num_train_epochs = 5, # Enable the Google Gemma 2B Model for n-shot learning, and allow the model to train on the wikipedia dataset 5 times.\n",
        "  fp16=True\n",
        ")\n",
        "\n",
        "# Use a data collator to batch the wikipedia dataset in preparation for training processes\n",
        "gemma2BDataColl = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Initialise the 'Trainer' tool, and input the parameters for the model to be trained on the tokenized wikipedia datasets, as well as the specific training parameter tunings.\n",
        "modelTrainerWP = Trainer(\n",
        "  model = loraModel,\n",
        "  args = trainingParamsWP,\n",
        "  train_dataset = wikipediaDSTTokenized,\n",
        "  eval_dataset = wikipediaDSVTokenized,\n",
        "  data_collator = gemma2BDataColl\n",
        ")\n",
        "\n",
        "# Perform the training processes for the Gemma 2B model on the tokenized wikipedia dataset.\n",
        "modelTrainerWP.train()"
      ],
      "metadata": {
        "id": "bY4QYXPqNnHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the Newly Modified Model (called GemmaPX (Post eXplainability)) to Google Drive to be Called Upon Later for Further Training Stages"
      ],
      "metadata": {
        "id": "PerZCVjS5Ib2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned Google Gemma 2B model and tokenizer to the local directory 'GemmaPX'\n",
        "loraModel.save_pretrained('/content/drive/My Drive/GemmaPX')\n",
        "tokenizer.save_pretrained('/content/drive/My Drive/GemmaPX')"
      ],
      "metadata": {
        "id": "79sib4NYMSNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Newly Modified Model (called GemmaPX (Post eXplainability)) from Google Drive to be Called Upon for Further Training Stages"
      ],
      "metadata": {
        "id": "BgtAaG_b6T4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Newly Modified Model (called GemmaPX (Post eXplainability)) from Google Drive to be Called Upon for Further Training Stages.\n",
        "#Create the configurations for the 'gemma_2b_en' model. To be used within improvement steps.\n",
        "modelConfig = AutoConfig.from_pretrained(\"google/gemma-2b\")\n",
        "\n",
        "# Edit/Tweak the 'gemma_2b_en' model configuration JSON file with new configurations, print the new model configurations, then set the model to use the newly updated configuration JSON file.\n",
        "modelConfig.attention_dropout = 0.1 #\n",
        "modelConfig.torch_dtype = \"float32\" #\n",
        "modelConfig.rms_norm_eps = 1e-05 #\n",
        "\n",
        "modelConfig.save_pretrained(\"/content/drive/My Drive/ConfigJSONGemma2B/\")\n",
        "\n",
        "modelConfigEdited = AutoConfig.from_pretrained(\"/content/drive/My Drive/ConfigJSONGemma2B/\")\n",
        "\n",
        "print(\"Edited/Tweaked Model Configurations:\")\n",
        "print(modelConfigEdited)\n",
        "\n",
        "# Load the fine-tuned Google Gemma 2B model and tokenizer from the local directory 'GemmaPX'\n",
        "model = AutoModelForCausalLM.from_pretrained('/content/drive/My Drive/GemmaPX', config=modelConfigEdited).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/My Drive/GemmaPX', use_fast=True)\n",
        "\n",
        "#Import the necessary libraries to use the PEFT fine-tuning tool.\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "#Setup the imported PEFT library for fine-tuning methods during training processes.\n",
        "loraConfigurations = LoraConfig(\n",
        "  task_type = TaskType.CAUSAL_LM,\n",
        "  r = 32,\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0.1,\n",
        "  target_modules = [\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "#Initialize a PEFT model using the Gemma 2B LLM model and the Lora Configuration variable.\n",
        "loraModel = get_peft_model(model, loraConfigurations).to(\"cuda\")"
      ],
      "metadata": {
        "id": "0aFpWwWX6mer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightEval MATH Dataset"
      ],
      "metadata": {
        "id": "dio4LhTRhteb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Google Gemma 2B model on the LightEval MATH dataset. Source: https://huggingface.co/datasets/lighteval/MATH\n",
        "# Retrieve and load the LightEval MATH dataset from HuggingFace with the 'all' subset.\n",
        "mathDS = load_dataset(\"lighteval/MATH\", \"all\", split='train', trust_remote_code=True)\n",
        "\n",
        "# Randomize the order of the entries within the LightEval MATH dataset, and use a seed so it can be reproduced.\n",
        "mathDS = mathDS.shuffle(seed=42)\n",
        "\n",
        "# Split the LightEval MATH dataset into train and validation sets (90% for train and 10% for validation) for the training processes.\n",
        "mathDSSplit = mathDS.train_test_split(test_size=0.1)\n",
        "mathDSTrain = mathDSSplit['train']\n",
        "mathDSValid = mathDSSplit['test']\n",
        "\n",
        "# Create a function which will tokenize the 'problem' and 'solution' sections of a LightEval MATH dataset entry.\n",
        "def tokenizeFunctionMATH(inputText):\n",
        "  qaString = str(inputText['problem']) + \" \" + str(inputText['solution'])\n",
        "  return tokenizer(qaString, padding='max_length', truncation=True, max_length = 350)\n",
        "\n",
        "# Tokenize the entire text sections of all the LightEval math train dataset entries, and map this to its own variable to be called upon later.\n",
        "mathDSTTokenized = mathDSTrain.map(tokenizeFunctionMATH)\n",
        "\n",
        "# Tokenize the entire text sections of all the LightEval validation dataset entries, and map this to its own variable to be called upon later.\n",
        "mathDSVTokenized = mathDSValid.map(tokenizeFunctionMATH)\n",
        "\n",
        "# Initialise the 'TrainingArguments' tool, and input specific arguments for how the model will be trained on the MATH dataset.\n",
        "trainingParamsMATH = TrainingArguments(\n",
        "  output_dir='/content/drive/My Drive/MATHTrainResults',\n",
        "  evaluation_strategy='epoch',\n",
        "  per_device_train_batch_size = 1,\n",
        "  gradient_accumulation_steps = 1,\n",
        "  num_train_epochs = 5, # Enable the Google Gemma 2B Model for n-shot learning, and allow the model to train on the math dataset 5 times.\n",
        "  fp16=True\n",
        ")\n",
        "\n",
        "# Use a data collator to batch the LightEval MATH dataset in preparation for training processes\n",
        "gemma2BDataColl = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Initialise the 'Trainer' tool, and input the parameters for the model to be trained on the tokenized MATH dataset, as well as the specific training parameter tunings.\n",
        "modelTrainerMATH = Trainer(\n",
        "  model = loraModel,\n",
        "  args = trainingParamsMATH,\n",
        "  train_dataset = mathDSTTokenized,\n",
        "  eval_dataset = mathDSVTokenized,\n",
        "  data_collator = gemma2BDataColl\n",
        ")\n",
        "\n",
        "# Perform the training processes for the Gemma 2B model on the tokenized MATH dataset.\n",
        "modelTrainerMATH.train()"
      ],
      "metadata": {
        "id": "JZU5AxYLhwim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CodeSearchNet Dataset"
      ],
      "metadata": {
        "id": "Zh0GO58x4dKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Google Gemma 2B model on the CodeSearchNet dataset. Source: https://huggingface.co/datasets/code-search-net/code_search_net\n",
        "# Retrieve and load the pre-processed CodeSearchNet dataset from HuggingFace with the 'train' subset.\n",
        "codesearchnetDS = load_dataset(\"code-search-net/code_search_net\", split=\"train\", trust_remote_code=True)\n",
        "\n",
        "# Randomize the order of the entries within the CodeSearchNet dataset, and use a seed so it can be reproduced.\n",
        "codesearchnetDS = codesearchnetDS.shuffle(seed=42)\n",
        "\n",
        "# Select 50000 examples from the randomized CodeSearchNet Dataset (for training time purposes with available resources)\n",
        "codesearchnetDS = codesearchnetDS.select(range(50000))\n",
        "\n",
        "# Split the CodeSearchNet dataset into train and validation sets (90% for train and 10% for validation) for the training processes.\n",
        "codesearchnetDSSplit = codesearchnetDS.train_test_split(test_size=0.1)\n",
        "codesearchnetDSTrain = codesearchnetDSSplit['train']\n",
        "codesearchnetDSValid = codesearchnetDSSplit['test']\n",
        "\n",
        "# Create a function which will tokenize the 'whole_func_string' section of a CodeSearchNet dataset entry.\n",
        "def tokenizeFunctionCSN(inputText):\n",
        "  return tokenizer(inputText['whole_func_string'], padding=True, truncation=True, max_length=350) # max_length implemented to save environment resources and stop resource errors.\n",
        "\n",
        "# Tokenize the entire text sections of all the CodeSearchNet train dataset entries, and map this to its own variable to be called upon later.\n",
        "codesearchnetDSTTokenized = codesearchnetDSTrain.map(tokenizeFunctionCSN, batched=True)\n",
        "\n",
        "# Tokenize the entire text sections of all the CodeSearchNet validation dataset entries, and map this to its own variable to be called upon later.\n",
        "codesearchnetDSVTokenized = codesearchnetDSValid.map(tokenizeFunctionCSN, batched=True)\n",
        "\n",
        "# Initialise the 'TrainingArguments' tool, and input specific arguments for how the model will be trained on the CodeSearchNet dataset.\n",
        "trainingParamsCSN = TrainingArguments(\n",
        "  output_dir='/content/drive/My Drive/CodeSearchNetTrainResults',\n",
        "  evaluation_strategy='epoch',\n",
        "  per_device_train_batch_size = 2,\n",
        "  gradient_accumulation_steps = 8,\n",
        "  num_train_epochs = 5, # Enable the Google Gemma 2B Model for n-shot learning, and allow the model to train on the CodeSearchNet dataset 5 times.\n",
        "  fp16=True\n",
        ")\n",
        "\n",
        "# Use a data collator to batch the CodeSearchNet dataset in preparation for training processes\n",
        "gemma2BDataColl = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Initialise the 'Trainer' tool, and input the parameters for the model to be trained on the tokenized CodeSearchNet dataset, as well as the specific training parameter tunings.\n",
        "modelTrainerCSN = Trainer(\n",
        "  model = loraModel,\n",
        "  args = trainingParamsCSN,\n",
        "  train_dataset = codesearchnetDSTTokenized,\n",
        "  eval_dataset = codesearchnetDSVTokenized,\n",
        "  data_collator = gemma2BDataColl\n",
        ")\n",
        "\n",
        "# Perform the training processes for the Gemma 2B model on the tokenized CodeSearchNet dataset.\n",
        "modelTrainerCSN.train()"
      ],
      "metadata": {
        "id": "SOnTjtzk9Hwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Improvements on the Google Gemma 2B Instruction-Tuned Model"
      ],
      "metadata": {
        "id": "ui_xS15sOXnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the Google Gemma 2B Instruction-Tuned Model"
      ],
      "metadata": {
        "id": "MDRrcxoRy9_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model, tokenizer and configurations for the 'gemma_instruct_2b_en' model. To be used within improvement steps.\n",
        "modelConfig2 = AutoConfig.from_pretrained(\"google/gemma-2b-it\")\n",
        "model2 = AutoModel.from_pretrained(\"google/gemma-2b-it\", config=modelConfig2)\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", use_fast=True)"
      ],
      "metadata": {
        "id": "psBAwYt41MCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvement 3 - Modifying/Tweaking the Architecture of the Google Gemma 2B Instruction-Tuned Model"
      ],
      "metadata": {
        "id": "n0ubMkHq5eWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the configuration settings and model architecture of the Google Gemma instruction-tuned 2B model"
      ],
      "metadata": {
        "id": "91EZsOuA6vpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print out the configuration settings and the architecture of the Google Gemma 2B instruction-tuned model for inspection purposes.\n",
        "print(\"Model Configurations:\")\n",
        "print(modelConfig2)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Model Architecture:\")\n",
        "summary(model2)"
      ],
      "metadata": {
        "id": "--b2Wmfx6lDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edit/Tweak the Google Gemma 2B instruction-tuned model configuration JSON file with new configurations"
      ],
      "metadata": {
        "id": "zRrWhoQ67Tpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the configurations for the 'gemma_instruct_2b_en' model. To be used within improvement steps.\n",
        "modelConfig2 = AutoConfig.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "# Edit/Tweak the 'gemma_instruct_2b_en' model configuration JSON file with new configurations, print the new model configurations, then set the model to use the newly updated configuration JSON file.\n",
        "modelConfig2.attention_dropout = 0.1 #\n",
        "modelConfig2.torch_dtype = \"float32\" #\n",
        "modelConfig2.rms_norm_eps = 1e-05 #\n",
        "\n",
        "modelConfig2.save_pretrained(\"/content/drive/My Drive/ConfigJSONGemmaIT2B/\")\n",
        "\n",
        "modelConfigEdited2 = AutoConfig.from_pretrained(\"/content/drive/My Drive/ConfigJSONGemmaIT2B/\")\n",
        "\n",
        "print(\"Edited/Tweaked Model Configurations:\")\n",
        "print(modelConfigEdited2)\n",
        "\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", config=modelConfigEdited2).to(\"cuda\")\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", use_fast=True)"
      ],
      "metadata": {
        "id": "mMnBXZHg7eI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvement 4 - Applying Fine-Tuning Methods/Techniques to the Google Gemma 2B Instruction-Tuned Model"
      ],
      "metadata": {
        "id": "d6RDOX77AnwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and Setup the PEFT library to use LoRA (Low-Rank Adoption)  "
      ],
      "metadata": {
        "id": "txuLzK_xA1Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import and setup the PEFT library in order to use the LoRA fine-tuning method during training processes. Source: https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
        "#Import the necessary libraries to use the PEFT fine-tuning tool.\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "#Setup the imported PEFT library for fine-tuning methods during training processes.\n",
        "loraConfigurations2 = LoraConfig(\n",
        "  task_type = TaskType.CAUSAL_LM,\n",
        "  r = 32,\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0.1,\n",
        "  target_modules = [\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "#Initialize a PEFT model using the Gemma 2B Instruction-Tuned LLM model and the Lora Configuration variable.\n",
        "loraModel2 = get_peft_model(model2, loraConfigurations2).to(\"cuda\")"
      ],
      "metadata": {
        "id": "Pi5jrfSGA8cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improvements 1 and 2 - Applying Additional Training & Enable N-Shot Learning for the Google Gemma 2B Instruction-Tuned Model"
      ],
      "metadata": {
        "id": "OD-t5uYaB3xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wikipedia Dataset"
      ],
      "metadata": {
        "id": "mE1kQ-L8B6Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Google Gemma 2B instruction-tuned model on the Wikipedia dataset. Source: https://huggingface.co/datasets/legacy-datasets/wikipedia\n",
        "# Retrieve and load a pre-processed version of the 'train' split of the Wikipedia dataset from HuggingFace.\n",
        "wikipediaDS2 = load_dataset(\"wikipedia\", \"20220301.en\", split='train', trust_remote_code=True)\n",
        "\n",
        "# Randomize the order of the entries within the Wikipedia dataset, and use a seed so it can be reproduced.\n",
        "wikipediaDS2 = wikipediaDS2.shuffle(seed=42)\n",
        "\n",
        "# Select 50000 examples from the randomized Wikipedia Dataset (for training time purposes with available resources)\n",
        "wikipediaDS2 = wikipediaDS2.select(range(50000))\n",
        "\n",
        "# Split the wikipedia dataset into train and validation sets (90% for train and 10% for validation) for the training processes.\n",
        "wikipediaDSSplit2 = wikipediaDS2.train_test_split(test_size=0.1)\n",
        "wikipediaDSTrain2 = wikipediaDSSplit2['train']\n",
        "wikipediaDSValid2 = wikipediaDSSplit2['test']\n",
        "\n",
        "# Create a function which will tokenize the 'text' section of a wikipedia dataset entry.\n",
        "def tokenizeFunctionWP2(inputText):\n",
        "  return tokenizer2(inputText['text'], padding=True, truncation=True, max_length=350) # max_length implemented to save environment resources and stop resource errors.\n",
        "\n",
        "# Tokenize the entire text sections of all the wikipedia train dataset entries, and map this to its own variable to be called upon later.\n",
        "wikipediaDSTTokenized2 = wikipediaDSTrain2.map(tokenizeFunctionWP2, batched=True)\n",
        "\n",
        "# Tokenize the entire text sections of all the wikipedia validation dataset entries, and map this to its own variable to be called upon later.\n",
        "wikipediaDSVTokenized2 = wikipediaDSValid2.map(tokenizeFunctionWP2, batched=True)\n",
        "\n",
        "# Initialise the 'TrainingArguments' tool, and input specific arguments for how the model will be trained on the wikipedia dataset.\n",
        "trainingParamsWP2 = TrainingArguments(\n",
        "  output_dir = '/content/drive/My Drive/WikipediaTrainResults2',\n",
        "  eval_strategy = 'epoch',\n",
        "  per_device_train_batch_size = 2,\n",
        "  gradient_accumulation_steps = 8,\n",
        "  num_train_epochs = 5, # Enable the Google Gemma 2B instruction-tuned Model for n-shot learning, and allow the model to train on the wikipedia dataset 5 times.\n",
        "  fp16=True\n",
        ")\n",
        "\n",
        "# Use a data collator to batch the wikipedia dataset in preparation for training processes\n",
        "gemma2BDataColl2 = DataCollatorForLanguageModeling(tokenizer=tokenizer2, mlm=False)\n",
        "\n",
        "# Initialise the 'Trainer' tool, and input the parameters for the model to be trained on the tokenized wikipedia datasets, as well as the specific training parameter tunings.\n",
        "modelTrainerWP2 = Trainer(\n",
        "  model = loraModel2,\n",
        "  args = trainingParamsWP2,\n",
        "  train_dataset = wikipediaDSTTokenized2,\n",
        "  eval_dataset = wikipediaDSVTokenized2,\n",
        "  data_collator = gemma2BDataColl2\n",
        ")\n",
        "\n",
        "# Perform the training processes for the Gemma 2B instruction-tuned model on the tokenized wikipedia dataset.\n",
        "modelTrainerWP2.train()"
      ],
      "metadata": {
        "id": "JnsRnhmYCVcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the Newly Modified Model (called GemmaITPX (Post eXplainability)) to Google Drive to be Called Upon Later for Further Training Stages"
      ],
      "metadata": {
        "id": "BuDK8RyLP3xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned Google Gemma 2B Instruction-Tuned model and tokenizer to the local directory 'GemmaITPX'\n",
        "loraModel2.save_pretrained('/content/drive/My Drive/GemmaITPX')\n",
        "tokenizer2.save_pretrained('/content/drive/My Drive/GemmaITPX')"
      ],
      "metadata": {
        "id": "E70yRioeQta6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Newly Modified Model (called GemmaITPX (Post eXplainability)) from Google Drive to be Called Upon for Further Training Stages"
      ],
      "metadata": {
        "id": "gMX533WcQ9UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Newly Modified Model (called GemmaITPX (Post eXplainability)) from Google Drive to be Called Upon for Further Training Stages.\n",
        "#Create the configurations for the 'gemma_instruct_2b_en' model. To be used within improvement steps.\n",
        "modelConfig2 = AutoConfig.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "# Edit/Tweak the 'gemma_instruct_2b_en' model configuration JSON file with new configurations, print the new model configurations, then set the model to use the newly updated configuration JSON file.\n",
        "modelConfig2.attention_dropout = 0.1 #\n",
        "modelConfig2.torch_dtype = \"float32\" #\n",
        "modelConfig2.rms_norm_eps = 1e-05 #\n",
        "\n",
        "modelConfig2.save_pretrained(\"/content/drive/My Drive/ConfigJSONGemmaIT2B/\")\n",
        "\n",
        "modelConfigEdited2 = AutoConfig.from_pretrained(\"/content/drive/My Drive/ConfigJSONGemmaIT2B/\")\n",
        "\n",
        "print(\"Edited/Tweaked Model Configurations:\")\n",
        "print(modelConfigEdited2)\n",
        "\n",
        "# Load the fine-tuned Google Gemma 2B instruction-tuned model and tokenizer from the local directory 'GemmaITPX'\n",
        "model2 = AutoModelForCausalLM.from_pretrained('/content/drive/My Drive/GemmaITPX', config=modelConfigEdited2).to(\"cuda\")\n",
        "tokenizer2 = AutoTokenizer.from_pretrained('/content/drive/My Drive/GemmaITPX', use_fast=True)\n",
        "\n",
        "#Import the necessary libraries to use the PEFT fine-tuning tool.\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "#Setup the imported PEFT library for fine-tuning methods during training processes.\n",
        "loraConfigurations2 = LoraConfig(\n",
        "  task_type = TaskType.CAUSAL_LM,\n",
        "  r = 32,\n",
        "  lora_alpha = 32,\n",
        "  lora_dropout = 0.1,\n",
        "  target_modules = [\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "#Initialize a PEFT model using the Gemma 2B Instruction-Tuned LLM model and the Lora Configuration variable.\n",
        "loraModel2 = get_peft_model(model2, loraConfigurations2).to(\"cuda\")"
      ],
      "metadata": {
        "id": "qn0xmHVtRHX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightEval MATH Dataset"
      ],
      "metadata": {
        "id": "8y88tuExFyXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Google Gemma 2B instruction-tuned model on the LightEval MATH dataset. Source: https://huggingface.co/datasets/lighteval/MATH\n",
        "# Retrieve and load the LightEval MATH dataset from HuggingFace with the 'all' subset.\n",
        "mathDS2 = load_dataset(\"lighteval/MATH\", \"all\", split='train', trust_remote_code=True)\n",
        "\n",
        "# Randomize the order of the entries within the LightEval MATH dataset, and use a seed so it can be reproduced.\n",
        "mathDS2 = mathDS2.shuffle(seed=42)\n",
        "\n",
        "# Split the LightEval MATH dataset into train and validation sets (90% for train and 10% for validation) for the training processes.\n",
        "mathDSSplit2 = mathDS2.train_test_split(test_size=0.1)\n",
        "mathDSTrain2 = mathDSSplit2['train']\n",
        "mathDSValid2 = mathDSSplit2['test']\n",
        "\n",
        "# Create a function which will tokenize the 'problem' and 'solution' sections of a LightEval MATH dataset entry.\n",
        "def tokenizeFunctionMATH2(inputText):\n",
        "  qaString2 = str(inputText['problem']) + \" \" + str(inputText['solution'])\n",
        "  return tokenizer2(qaString2, padding='max_length', truncation=True, max_length=350)\n",
        "\n",
        "# Tokenize the entire text sections of all the LightEval math train dataset entries, and map this to its own variable to be called upon later.\n",
        "mathDSTTokenized2 = mathDSTrain2.map(tokenizeFunctionMATH2)\n",
        "\n",
        "# Tokenize the entire text sections of all the LightEval validation dataset entries, and map this to its own variable to be called upon later.\n",
        "mathDSVTokenized2 = mathDSValid2.map(tokenizeFunctionMATH2)\n",
        "\n",
        "# Initialise the 'TrainingArguments' tool, and input specific arguments for how the model will be trained on the MATH dataset.\n",
        "trainingParamsMATH2 = TrainingArguments(\n",
        "  output_dir='/content/drive/My Drive/MATHTrainResults2',\n",
        "  evaluation_strategy='epoch',\n",
        "  per_device_train_batch_size = 1,\n",
        "  gradient_accumulation_steps = 1,\n",
        "  num_train_epochs = 5, # Enable the Google Gemma 2B instruction-tuned Model for n-shot learning, and allow the model to train on the math dataset 5 times.\n",
        "  fp16=True\n",
        ")\n",
        "\n",
        "# Use a data collator to batch the LightEval MATH dataset in preparation for training processes\n",
        "gemma2BDataColl2 = DataCollatorForLanguageModeling(tokenizer=tokenizer2, mlm=False)\n",
        "\n",
        "# Initialise the 'Trainer' tool, and input the parameters for the model to be trained on the tokenized MATH dataset, as well as the specific training parameter tunings.\n",
        "modelTrainerMATH2 = Trainer(\n",
        "  model = loraModel2,\n",
        "  args = trainingParamsMATH2,\n",
        "  train_dataset = mathDSTTokenized2,\n",
        "  eval_dataset = mathDSVTokenized2,\n",
        "  data_collator = gemma2BDataColl2\n",
        ")\n",
        "\n",
        "# Perform the training processes for the Gemma 2B instruction-tuned model on the tokenized MATH dataset.\n",
        "modelTrainerMATH2.train()"
      ],
      "metadata": {
        "id": "Sua-Pp7LFzgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CodeSearchNet Dataset"
      ],
      "metadata": {
        "id": "Hlw9AhNlGTzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Google Gemma 2B instruction-tuned model on the CodeSearchNet dataset. Source: https://huggingface.co/datasets/code-search-net/code_search_net\n",
        "# Retrieve and load the pre-processed CodeSearchNet dataset from HuggingFace with the 'train' subset.\n",
        "codesearchnetDS2 = load_dataset(\"code-search-net/code_search_net\", split=\"train\", trust_remote_code=True)\n",
        "\n",
        "# Randomize the order of the entries within the CodeSearchNet dataset, and use a seed so it can be reproduced.\n",
        "codesearchnetDS2 = codesearchnetDS2.shuffle(seed=42)\n",
        "\n",
        "# Select 50000 examples from the randomized CodeSearchNet Dataset (for training time purposes with available resources)\n",
        "codesearchnetDS2 = codesearchnetDS2.select(range(50000))\n",
        "\n",
        "# Split the CodeSearchNet dataset into train and validation sets (90% for train and 10% for validation) for the training processes.\n",
        "codesearchnetDSSplit2 = codesearchnetDS2.train_test_split(test_size=0.1)\n",
        "codesearchnetDSTrain2 = codesearchnetDSSplit2['train']\n",
        "codesearchnetDSValid2 = codesearchnetDSSplit2['test']\n",
        "\n",
        "# Create a function which will tokenize the 'whole_func_string' section of a CodeSearchNet dataset entry.\n",
        "def tokenizeFunctionCSN2(inputText):\n",
        "  return tokenizer2(inputText['whole_func_string'], padding=True, truncation=True, max_length=350) # max_length implemented to save environment resources and stop resource errors.\n",
        "\n",
        "# Tokenize the entire text sections of all the CodeSearchNet train dataset entries, and map this to its own variable to be called upon later.\n",
        "codesearchnetDSTTokenized2 = codesearchnetDSTrain2.map(tokenizeFunctionCSN2, batched=True)\n",
        "\n",
        "# Tokenize the entire text sections of all the CodeSearchNet validation dataset entries, and map this to its own variable to be called upon later.\n",
        "codesearchnetDSVTokenized2 = codesearchnetDSValid2.map(tokenizeFunctionCSN2, batched=True)\n",
        "\n",
        "# Initialise the 'TrainingArguments' tool, and input specific arguments for how the model will be trained on the CodeSearchNet dataset.\n",
        "trainingParamsCSN2 = TrainingArguments(\n",
        "  output_dir='/content/drive/My Drive/CodeSearchNetTrainResults2',\n",
        "  evaluation_strategy='epoch',\n",
        "  per_device_train_batch_size = 2,\n",
        "  gradient_accumulation_steps = 8,\n",
        "  num_train_epochs = 5, # Enable the Google Gemma 2B instruction-tuned Model for n-shot learning, and allow the model to train on the CodeSearchNet dataset 5 times.\n",
        "  fp16=True\n",
        ")\n",
        "\n",
        "# Use a data collator to batch the CodeSearchNet dataset in preparation for training processes\n",
        "gemma2BDataColl2 = DataCollatorForLanguageModeling(tokenizer=tokenizer2, mlm=False)\n",
        "\n",
        "# Initialise the 'Trainer' tool, and input the parameters for the model to be trained on the tokenized CodeSearchNet dataset, as well as the specific training parameter tunings.\n",
        "modelTrainerCSN2 = Trainer(\n",
        "  model = loraModel2,\n",
        "  args = trainingParamsCSN2,\n",
        "  train_dataset = codesearchnetDSTTokenized2,\n",
        "  eval_dataset = codesearchnetDSVTokenized2,\n",
        "  data_collator = gemma2BDataColl2\n",
        ")\n",
        "\n",
        "# Perform the training processes for the Gemma 2B model on the tokenized CodeSearchNet dataset.\n",
        "modelTrainerCSN2.train()"
      ],
      "metadata": {
        "id": "gIbbPj8eGeMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gather Benchmark Results/Performance Metrics of the Selected Google Gemma LLM Models After XAI Implementation**"
      ],
      "metadata": {
        "id": "iLsc3y7vbADQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Google Gemma LLM models to be evaluated, this step allows the modified models to function with the DeepEval library"
      ],
      "metadata": {
        "id": "a0PPzX68bf0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup the modified Google Gemma LLM models to be evaluated for benchmarking. Source:https://docs.confident-ai.com/docs/benchmarks-introduction#benchmarking-your-llm\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "\n",
        "class Gemma2BENPX(DeepEvalBaseLLM):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def load_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        model = self.load_model()\n",
        "\n",
        "        device = \"cuda\" # Load the device onto a CUDA GPU for faster processing\n",
        "\n",
        "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "        model.to(device)\n",
        "\n",
        "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
        "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        return self.generate(prompt)\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return \"Gemma 2B EN PX\"\n",
        "\n",
        "#Create the model and tokenizer for the 'Gemma PX' model.\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"/content/drive/My Drive/GemmaPX\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/GemmaPX\")\n",
        "\n",
        "#Create the model and tokenizer for the 'Gemma IT PX' modified model\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\"/content/drive/My Drive/GemmaITPX\")\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/GemmaITPX\")\n",
        "\n",
        "#Create the models using the deepeval class and defined function\n",
        "#gemma_PX = Gemma2BENPX(model=model, tokenizer=tokenizer)\n",
        "gemma_IT_PX = Gemma2BENPX(model=model2, tokenizer=tokenizer2)"
      ],
      "metadata": {
        "id": "NeFKLtD_bekk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the necessary libraries from the deepeval package"
      ],
      "metadata": {
        "id": "x7ATVXsFlcs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the necessary benchmark tools and tasks from the deepeval library. Source:https://docs.confident-ai.com/docs/getting-started\n",
        "from deepeval.benchmarks import MMLU, HellaSwag, BigBenchHard, TruthfulQA, GSM8K #Import the benchmark type\n",
        "from deepeval.benchmarks.tasks import MMLUTask, HellaSwagTask, BigBenchHardTask, TruthfulQATask #Import the tasks for each benchmark type\n",
        "from deepeval.benchmarks.modes import TruthfulQAMode #Import the QAmode to be used on TruthfulQA benchmark."
      ],
      "metadata": {
        "id": "YMcXLb-KllEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GSM8K Benchmark"
      ],
      "metadata": {
        "id": "Jl4XFIilpNN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the GSM8K benchmark to be performed on the modified Google Gemma LLM models."
      ],
      "metadata": {
        "id": "1vudFQGLqZ21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define benchmark with specific tasks and shots for GSM8K. Source:https://docs.confident-ai.com/docs/benchmarks-gsm8k\n",
        "#To be used on the 'Gemma PX' model.\n",
        "benchmarkGSM8K = GSM8K(\n",
        "    n_problems=1319, #1319 is all problems available for the GSM8K benchmark.\n",
        "    n_shots=1, #Go through all problems only once (1-shot learning).\n",
        "    enable_cot=False #Disable Chain-of-Thoughts (CoT) prompting.\n",
        ")\n",
        "\n",
        "#To be used on the 'Gemma IT PX' model.\n",
        "benchmark2GSM8K = GSM8K(\n",
        "    n_problems=1319, #1319 is all problems available for the GSM8K benchmark.\n",
        "    n_shots=1, #Go through all problems only once (1-shot learning).\n",
        "    enable_cot=False #Disable Chain-of-Thoughts (CoT) prompting.\n",
        ")"
      ],
      "metadata": {
        "id": "8nAJh-R-pahV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the GSM8K benchmark evaluation on the 'Gemma PX' model, and print out the score."
      ],
      "metadata": {
        "id": "i5N26OtbqeHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the GSM8K benchmark on the modified 'Gemma PX' Google Gemma LLM model and print the results\n",
        "benchmarkGSM8K.evaluate(model=gemma_PX)\n",
        "print(benchmarkGSM8K.overall_score)"
      ],
      "metadata": {
        "id": "odmppJRrqsOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the GSM8K benchmark evaluation on the 'Gemma IT PX' model, and print out the score."
      ],
      "metadata": {
        "id": "5fMW63yNq9SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the GSM8K benchmark on the modified 'Gemma IT PX' Google Gemma LLM model and print the results\n",
        "benchmark2GSM8K.evaluate(model=gemma_IT_PX)\n",
        "print(benchmark2GSM8K.overall_score)"
      ],
      "metadata": {
        "id": "yUWcWVkLrJUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TruthfulQA Benchmark"
      ],
      "metadata": {
        "id": "agd6jX0iKNa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the TruthfulQA benchmark to be performed on the modified Google Gemma LLM models."
      ],
      "metadata": {
        "id": "4eAKFVbEKO8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define benchmark with specific tasks and modes for TruthfulQA. Source:https://docs.confident-ai.com/docs/benchmarks-truthful-qa\n",
        "#To be used on the 'Gemma PX' model.\n",
        "benchmarkTruthfulQA = TruthfulQA(\n",
        "    mode=TruthfulQAMode.MC2 #Perform all 817 tasks under MC2 benchmarking mode\n",
        ")\n",
        "\n",
        "#To be used on the 'Gemma IT PX' model.\n",
        "benchmark2TruthfulQA = TruthfulQA(\n",
        "    mode=TruthfulQAMode.MC2 #Perform all 817 tasks under MC2 benchmarking mode\n",
        ")"
      ],
      "metadata": {
        "id": "MTte_4p3KfQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the TruthfulQA benchmark evaluation on the modified 'Gemma PX' model, and print out the score."
      ],
      "metadata": {
        "id": "H2gCXkA7K0im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the HumanEval benchmark on the modified 'Gemma PX' Google Gemma LLM model and print the results\n",
        "benchmarkTruthfulQA.evaluate(model=gemma_PX)\n",
        "print(benchmarkTruthfulQA.overall_score)"
      ],
      "metadata": {
        "id": "iZoaL9AVLE4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the TruthfulQA benchmark evaluation on the modified 'Gemma IT PX' model, and print out the score."
      ],
      "metadata": {
        "id": "xu6r3eBSMHZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the HumanEval benchmark on the modified 'Gemma IT PX' Google Gemma LLM model and print the results\n",
        "benchmark2TruthfulQA.evaluate(model=gemma_IT_PX)\n",
        "print(benchmark2TruthfulQA.overall_score)"
      ],
      "metadata": {
        "id": "6B-6cYLYNnZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigBenchHard (BBH) Benchmark"
      ],
      "metadata": {
        "id": "uZ4iTLM39Hw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the BigBenchHard (BBH) benchmark to be performed on the modified Google Gemma LLM models."
      ],
      "metadata": {
        "id": "AghbPbXf9TPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define benchmark with specific tasks and shots for BigBenchHard (BBH). Source:https://docs.confident-ai.com/docs/benchmarks-big-bench-hard\n",
        "#To be used on the 'Gemma PX' model.\n",
        "benchmarkBBH = BigBenchHard(\n",
        "    tasks=[BigBenchHardTask.BOOLEAN_EXPRESSIONS, BigBenchHardTask.CAUSAL_JUDGEMENT, BigBenchHardTask.DATE_UNDERSTANDING, BigBenchHardTask.DISAMBIGUATION_QA, BigBenchHardTask.FORMAL_FALLACIES], #Define the tasks to perform for the BBH benchmark on the 'Gemma PX' model.\n",
        "    n_shots=1, #Go through each question once (1-shot learning).\n",
        "    enable_cot=False #Disable Chain-of-Thoughts (CoT)\n",
        ")\n",
        "\n",
        "#To be used on the 'Gemma IT PX' model.\n",
        "benchmark2BBH = BigBenchHard(\n",
        "    tasks=[BigBenchHardTask.BOOLEAN_EXPRESSIONS, BigBenchHardTask.CAUSAL_JUDGEMENT, BigBenchHardTask.DATE_UNDERSTANDING, BigBenchHardTask.DISAMBIGUATION_QA, BigBenchHardTask.FORMAL_FALLACIES], #Define the tasks to perform for the BBH benchmark on the 'Gemma IT PX' model.\n",
        "    n_shots=1, #Go through each question once (1-shot learning).\n",
        "    enable_cot=False #Disable Chain-of-Thoughts (CoT)\n",
        ")"
      ],
      "metadata": {
        "id": "C6sHxrxW-kZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the BBH benchmark evaluation on the 'Gemma PX' model, and print out the score."
      ],
      "metadata": {
        "id": "_OXhGSXgeMB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the BBH benchmark on the 'Gemma PX' Google Gemma LLM model and print the results\n",
        "benchmarkBBH.evaluate(model=gemma_PX)\n",
        "print(benchmarkBBH.overall_score)"
      ],
      "metadata": {
        "id": "Z2Y4ccJzeUVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the BBH benchmark evaluation on the 'Gemma IT PX' model, and print out the score."
      ],
      "metadata": {
        "id": "DRujaflgewQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the BBH benchmark on the 'Gemma IT PX' Google Gemma LLM model and print the results\n",
        "benchmark2BBH.evaluate(model=gemma_IT_PX)\n",
        "print(benchmark2BBH.overall_score)"
      ],
      "metadata": {
        "id": "SsXc5EDsfArs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HellaSwag Benchmark"
      ],
      "metadata": {
        "id": "11A8acvXUoso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the HellaSwag benchmark to be performed on the modified Google Gemma LLM models."
      ],
      "metadata": {
        "id": "lUM69s1UUq5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define benchmark with specific tasks and shots for HellaSwag. Source:https://docs.confident-ai.com/docs/benchmarks-hellaswag\n",
        "#To be used on the 'Gemma PX' model.\n",
        "benchmarkHellaSwag = HellaSwag(\n",
        "    tasks=[HellaSwagTask.TRIMMING_BRANCHES_OR_HEDGES, HellaSwagTask.WASHING_HANDS, HellaSwagTask.PLAYING_POOL, HellaSwagTask.ZUMBA, HellaSwagTask.CRICKET, HellaSwagTask.BATON_TWIRLING, HellaSwagTask.PHILOSOPHY_AND_RELIGION, HellaSwagTask.GROOMING_DOG, HellaSwagTask.FIXING_THE_ROOF, HellaSwagTask.FIXING_BICYCLE], #Define the tasks to perform for the HellaSwag benchmark on the 'Gemma PX' model.\n",
        "    n_shots=1 #Go through each question once (1-shot learning).\n",
        ")\n",
        "\n",
        "#To be used on the 'Gemma IT PX' model.\n",
        "benchmark2HellaSwag = HellaSwag(\n",
        "    tasks=[HellaSwagTask.TRIMMING_BRANCHES_OR_HEDGES, HellaSwagTask.WASHING_HANDS, HellaSwagTask.PLAYING_POOL, HellaSwagTask.ZUMBA, HellaSwagTask.CRICKET, HellaSwagTask.BATON_TWIRLING, HellaSwagTask.PHILOSOPHY_AND_RELIGION, HellaSwagTask.GROOMING_DOG, HellaSwagTask.FIXING_THE_ROOF, HellaSwagTask.FIXING_BICYCLE], #Define the tasks to perform for the HellaSwag benchmark on the 'Gemma IT PX' model.\n",
        "    n_shots=1 #Go through each question once (1-shot learning).\n",
        ")"
      ],
      "metadata": {
        "id": "g55NzH9zUqM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the HellaSwag benchmark evaluation on the 'Gemma PX' model, and print out the score."
      ],
      "metadata": {
        "id": "9eBaUbPLZoo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the HellaSwag benchmark on the 'Gemma PX' Google Gemma LLM model and print the results\n",
        "benchmarkHellaSwag.evaluate(model=gemma_PX)\n",
        "print(benchmarkHellaSwag.overall_score)"
      ],
      "metadata": {
        "id": "FPMmcsouZuLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the HellaSwag benchmark evaluation on the 'Gemma IT PX' model, and print out the score."
      ],
      "metadata": {
        "id": "-gBpC5SUaKMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the HellaSwag benchmark on the 'Gemma IT PX' Google Gemma LLM model and print the results\n",
        "benchmark2HellaSwag.evaluate(model=gemma_IT_PX)\n",
        "print(benchmark2HellaSwag.overall_score)"
      ],
      "metadata": {
        "id": "jJNGpzimaW1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MMLU Benchmark"
      ],
      "metadata": {
        "id": "5ZGpJXSqhElA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the MMLU benchmark to be performed on the modified Google Gemma LLM models."
      ],
      "metadata": {
        "id": "Plw0X0DihJAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define benchmark with specific tasks and shots for Massive Multitask Language Understanding (MMLU). Source:https://docs.confident-ai.com/docs/benchmarks-mmlu\n",
        "#To be used on the 'Gemma PX' model.\n",
        "benchmarkMMLU = MMLU(\n",
        "    tasks=[MMLUTask.BUSINESS_ETHICS, MMLUTask.HIGH_SCHOOL_PHYSICS, MMLUTask.HIGH_SCHOOL_WORLD_HISTORY, MMLUTask.HIGH_SCHOOL_MICROECONOMICS, MMLUTask.HIGH_SCHOOL_BIOLOGY, MMLUTask.PHILOSOPHY, MMLUTask.ANATOMY, MMLUTask.COLLEGE_CHEMISTRY, MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ELECTRICAL_ENGINEERING], #Define the tasks to perform for the MMLU benchmark on the 'Gemma PX' model.\n",
        "    n_shots=1 #Go through each question once (1-shot learning).\n",
        ")\n",
        "\n",
        "#To be used on the 'gemma_instruct_2b_en' model.\n",
        "benchmark2MMLU = MMLU(\n",
        "    tasks=[MMLUTask.BUSINESS_ETHICS, MMLUTask.HIGH_SCHOOL_PHYSICS, MMLUTask.HIGH_SCHOOL_WORLD_HISTORY, MMLUTask.HIGH_SCHOOL_MICROECONOMICS, MMLUTask.HIGH_SCHOOL_BIOLOGY, MMLUTask.PHILOSOPHY, MMLUTask.ANATOMY, MMLUTask.COLLEGE_CHEMISTRY, MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ELECTRICAL_ENGINEERING], #Define the tasks to perform for the MMLU benchmark on the 'Gemma IT PX' model.\n",
        "    n_shots=1 #Go through each question once (1-shot learning).\n",
        ")"
      ],
      "metadata": {
        "id": "z_o_dw4XhVTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the MMLU benchmark evaluation on the modified 'Gemma PX' model, and print out the score."
      ],
      "metadata": {
        "id": "KSegR-YUjmln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the MMLU benchmark evaluation on the 'Gemma PX' model, and print out the score.\n",
        "benchmarkMMLU.evaluate(model=gemma_PX)\n",
        "print(benchmarkMMLU.overall_score)"
      ],
      "metadata": {
        "id": "c-AGpB3rqUN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the MMLU benchmark evaluation on the modified 'Gemma IT PX' model, and print out the score."
      ],
      "metadata": {
        "id": "8_SJZAVWrspo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the MMLU benchmark evaluation on the 'Gemma IT PX' model, and print out the score.\n",
        "benchmark2MMLU.evaluate(model=gemma_IT_PX)\n",
        "print(benchmark2MMLU.overall_score)"
      ],
      "metadata": {
        "id": "ywIsfC0xr1ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apply XAI methods to the Selected Google Gemma Models for Post-XAI Modification Explanations Using the Hugging Face API.**"
      ],
      "metadata": {
        "id": "Ve_tfz8DfkxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modified Google Gemma models using Google Drive, and create a list of 15 questions in various categories."
      ],
      "metadata": {
        "id": "pfKjyBbvfnCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a list of 15 questions in different categories, the different XAI tools and methods will use these questions to evaluate the modified Google Gemma LLM models. The questions have been spread over 5 different categories and each question has a different level of complexity, this ensures the modified Google Gemma LLM models are evaluated thoroughly."
      ],
      "metadata": {
        "id": "cpuK1_CEmcWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model and tokenizer for the 'Gemma PX' model. Comment out when necessary to use/unuse the model (for resource reasons). To be used with the Bertviz attention-visualisation and HotpotQA XAI tools.\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"/content/drive/My Drive/GemmaPX\", output_attentions=True, attn_implementation=\"eager\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/GemmaPX\", use_fast=True)\n",
        "#model.config.is_decoder = True\n",
        "\n",
        "#Create the model and tokenizer for the 'Gemma PX' model. Comment out when necessary to use/unuse the model (for resource reasons). To be used with the LIME XAI tool.\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/My Drive/GemmaPX\", num_labels=2)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/GemmaPX\", use_fast=True)\n",
        "\n",
        "#Create the model and tokenizer for the 'Gemma IT PX' model. Comment out when necessary to use/unuse the model (for resource reasons). To be used with the Bertviz attention-visualisation and HotpotQA XAI tools.\n",
        "#model2 = AutoModelForCausalLM.from_pretrained(\"/content/drive/My Drive/GemmaITPX\", output_attentions=True, attn_implementation=\"eager\")\n",
        "#tokenizer2 = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/GemmaITPX\", use_fast=True)\n",
        "#model2.config.is_decoder = True\n",
        "\n",
        "#Create the model and tokenizer for the 'Gemma IT PX' model. Comment out when necessary to use/unuse the model (for resource reasons). To be used with the LIME XAI tool.\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/My Drive/GemmaITPX\", num_labels=2)\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/GemmaITPX\", use_fast=True)\n",
        "\n",
        "#15 questions in total will be asked over general, maths, science, programming/code and paradoxical categories.\n",
        "questions = [] #Create a new list to hold the text questions\n",
        "#3 General Questions\n",
        "questions.append(\"How many minutes would it take me to walk from the Northumbria University campus located in Newcastle upon Tyne, to the city centre located in Newcastle upon Tyne?\") #Question 1\n",
        "questions.append(\"To date, who is the most decorated Olympian?\") #Question 2\n",
        "questions.append(\"Can you list the 'New 7 Wonders of the World'?\") #Question 3\n",
        "#3 Maths Questions\n",
        "questions.append(\"What is the value of 'x' in this equation: 18 + 8x = 30\") #Question 4\n",
        "questions.append(\"What is the limit of 'sin(x)/x' as x approaches 0?\") #Question 5\n",
        "questions.append(\"What is the value of 'Pi' to 30 decimal places?\") #Question 6\n",
        "#3 Science Questions\n",
        "questions.append(\"What happens to a star once it reaches the end of its life?\") #Question 7\n",
        "questions.append(\"What is the half-life of hydrogen-3 (tritium)?\") #Question 8\n",
        "questions.append(\"What is 'CRISPR' technology used for?\") #Question 9\n",
        "#3 Programming/Code Questions\n",
        "questions.append(\"Create a pseudocode method/function that checks to see if a number is either even or odd.\") #Question 10\n",
        "questions.append(\"What is the time complexity and space complexity of the merge sort algorithm?\") #Question 11\n",
        "questions.append(\"What are the key differences between the programming languages 'Python', 'C' and 'Assembly'?\") #Question 12\n",
        "#3 Paradoxical Questions\n",
        "questions.append(\"Is the sentence 'This sentence is false.' true or false?\") #Question 13\n",
        "questions.append(\"If there was a set of all sets, would that set contain itself?\") #Question 14\n",
        "questions.append(\"What happens when an unstoppable object collides with an immovable object?\") #Question 15"
      ],
      "metadata": {
        "id": "BZFo5E5VhPeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply General (Model-Agnostic) XAI methods to the Gemma PX and Gemma IT PX Instruction-Tuned models"
      ],
      "metadata": {
        "id": "hvr1br1aYG5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIME Explanations for the Gemma PX Model"
      ],
      "metadata": {
        "id": "Xqyx1f0iYTgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIME stands for \"Local Interpretable Model-agnostic Explanations\" and this XAI tool can be used to give explanations to all kinds of machine learning models/AI systems, regardless of their complexity and purpose. The LIME XAI tool aims to give understanding to machine learning models/AI systems by interpreting their behaviour within a specific and particular instance, and using simple model structures to approximate values and behaviour.\n",
        "\n",
        "With textual models and data, LIME identifies which tokens contributes the most within an input to an LLM model by changing and removing different tokens in an input. The \"LimeTextExplaier\" library will be used, this library functions by applying an exponential kernel on cosine distance, and restricting explanations to words that are present in documents. The LIME XAI tool will be used on the Gemma PX model with the list of 15 questions created for evaluation and investigation."
      ],
      "metadata": {
        "id": "131zEpg5dG8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LIME Explanations to be used on the Gemma PX model. Source: https://lime-ml.readthedocs.io/en/latest/lime.html\n",
        "#Import the LIME \"Text Explainer\" library to use the LIME XAI tool on the Gemma PX LLM model\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "#Define a function which predicts a tensorflow value for each token in an input being fed into the Gemma PX model, to be used with the LIME text explainer.\n",
        "def gemmapx_model_predictions(question):\n",
        "  modelInputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True)\n",
        "  modelOutputs = model(**modelInputs)\n",
        "  modelLogits = modelOutputs.logits\n",
        "  modelProbabilities = torch.nn.functional.softmax(modelLogits, dim=-1).detach().numpy()\n",
        "  return modelProbabilities\n",
        "\n",
        "#With every question contained in the question array, use the LIME XAI tool/method and output the results\n",
        "LIMEexplainer = LimeTextExplainer(class_names=['Negative', 'Positive'])\n",
        "for question in questions:\n",
        "  explanation = LIMEexplainer.explain_instance(question, gemmapx_model_predictions, labels=[0, 1], num_samples=100)\n",
        "  explanation.show_in_notebook(text=question)"
      ],
      "metadata": {
        "id": "CpG_1aBYYQD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIME Explanations for the Gemma IT PX Model"
      ],
      "metadata": {
        "id": "qCoWAiVmdrdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIME stands for \"**L**ocal **I**nterpretable **M**odel-agnostic **E**xplanations\" and this XAI tool can be used to give explanations to all kinds of machine learning models/AI systems, regardless of their complexity and purpose. The LIME XAI tool aims to give understanding to machine learning models/AI systems by interpreting their behaviour within a specific and particular instance, and using simple model structures to approximate values and behaviour.\n",
        "\n",
        "With textual models and data, LIME identifies which tokens contributes the most within an input to an LLM model by changing and removing different tokens in an input. The \"LimeTextExplaier\" library will be used, this library functions by applying an exponential kernel on cosine distance, and restricting explanations to words that are present in documents. The LIME XAI tool will be used on the Gemma IT PX model with the list of 15 questions created for evaluation and investigation."
      ],
      "metadata": {
        "id": "23CZ8-dgd4Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIME Explanations to be used on the Gemma IT PX model. Source: https://lime-ml.readthedocs.io/en/latest/lime.html\n",
        "#Import the necessary libraries to use the LIME XAI tool on the Gemma IT PX LLM model\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "#Define a function which predicts a tensorflow value for each token in an input being fed into the Gemma IT PX model, to be used with the LIME text explainer.\n",
        "def gemmaitpx_model_predictions(question):\n",
        "  modelInputs2 = tokenizer2(question, return_tensors='pt', padding=True, truncation=True)\n",
        "  modelOutputs2 = model2(**modelInputs2)\n",
        "  modelLogits2 = modelOutputs2.logits\n",
        "  modelProbabilities2 = torch.nn.functional.softmax(modelLogits2, dim=-1).detach().numpy()\n",
        "  return modelProbabilities2\n",
        "\n",
        "#With every question contained in the question array, use the LIME XAI tool/method and output the results\n",
        "LIMEexplainer2 = LimeTextExplainer(class_names=['Negative', 'Positive'])\n",
        "for question in questions:\n",
        "  explanation2 = LIMEexplainer2.explain_instance(question, gemmaitpx_model_predictions, labels=[0, 1], num_samples=100)\n",
        "  explanation2.show_in_notebook(text=question)"
      ],
      "metadata": {
        "id": "7WyG6ZmSeHsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Model-Specific XAI methods to the Gemma PX and Gemma IT PX Models (LLM/Deep Learning)"
      ],
      "metadata": {
        "id": "wpVC97rwmsIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention-Visualization Explanations for the Gemma PX Model"
      ],
      "metadata": {
        "id": "mZLt0X9Gmtgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BertViz attention-visualisation XAI tool is an open-source and interactive lens that allows users/developers to peek inside the workings of machine learning models and AI systems, specifically deep learning models and neural network models. The BertViz attention-visualisation XAI tool aims to give understanding to deep learning/neural network models by translating numerical data into diagrams which can be interpreted for investigation and analysis.\n",
        "\n",
        "The BertViz attention-visualisation XAI tool has 3 different flavours/views available to investigate and analyze deep learning/neural network models. There is the 'head view' which allows users/developers to view attention for 1 or more attention heads in the same layer of a model, the 'model view' which allows users/developers to view attention across all layers and heads within a model, and the 'neuron view' which allows users/developers to view attention for individual neurons within a model. The BertViz attention-visualisation XAI tool will be used on the Gemma PX model with the list of 15 questions created, and with the 'model view' for evaluation and investigation."
      ],
      "metadata": {
        "id": "27wMuXaZnmVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the BertViz Attention-Visualisation library on the Gemma PX model. Source: https://pypi.org/project/bertviz/\n",
        "# Import the \"model_view\" library from BertViz, to be used on the Gemma PX model\n",
        "from bertviz import model_view\n",
        "\n",
        "# For each question in the list of questions, tokenize the question, feed it into the Gemma PX model to obtain outputs, then apply the BertViz \"model_view\" XAI tool to obtain attention-visualization outputs\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    attention = outputs.attentions\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "    model_view(attention, tokens)"
      ],
      "metadata": {
        "id": "5ELZuTGCpGN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention-Visualization Explanations for the Gemma IT PX Model"
      ],
      "metadata": {
        "id": "rLeqTE68y9yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BertViz attention-visualisation XAI tool is an open-source and interactive lens that allows users/developers to peek inside the workings of machine learning models and AI systems, specifically deep learning models and neural network models. The BertViz attention-visualisation XAI tool aims to give understanding to deep learning/neural network models by translating numerical data into diagrams which can be interpreted for investigation and analysis.\n",
        "\n",
        "The BertViz attention-visualisation XAI tool has 3 different flavours/views available to investigate and analyze deep learning/neural network models. There is the 'head view' which allows users/developers to view attention for 1 or more attention heads in the same layer of a model, the 'model view' which allows users/developers to view attention across all layers and heads within a model, and the 'neuron view' which allows users/developers to view attention for individual neurons within a model. The BertViz attention-visualisation XAI tool will be used on the Gemma IT PX model with the list of 15 questions created, and with the 'model view' for evaluation and investigation."
      ],
      "metadata": {
        "id": "5QXoPXNj1DsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the BertViz Attention-Visualisation library on the Gemma IT PX model. Source: https://pypi.org/project/bertviz/\n",
        "# Import the \"model_view\" library from BertViz, to be used on the Gemma IT PX model\n",
        "from bertviz import model_view\n",
        "\n",
        "# For each question in the list of questions, tokenize the question, feed it into the Gemma IT PX model to obtain outputs, then apply the BertViz \"model_view\" XAI tool to obtain attention-visualization outputs\n",
        "for question in questions:\n",
        "    inputs2 = tokenizer2(question, return_tensors='pt')\n",
        "    outputs2 = model2(**inputs2)\n",
        "    attention2 = outputs2.attentions\n",
        "    tokens2 = tokenizer2.convert_ids_to_tokens(inputs2['input_ids'][0])\n",
        "    model_view(attention2, tokens2)"
      ],
      "metadata": {
        "id": "ZuE88VwO1RsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply XAI methods to Data/Datasets being fed into the Gemma PX and Gemma IT PX models"
      ],
      "metadata": {
        "id": "1yVRGGxKUX4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HotpotQA Explanations for the Gemma PX Model"
      ],
      "metadata": {
        "id": "wytGTLyIUcPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The HotpotQA XAI tool is a question-answering dataset, created by a collective team of NLP researchers at Carnegie Mellon University, Stanford University, and Université de Montréal. The HotpotQA XAI tool question-answering dataset is Wikipedia-based, with 113k question-answer pairs making up its entirety. These question-answer pairs are split up into various partitions, a 'train' partition which contains 90447 pairs, a 'validation' partition which contains 7405 pairs and a 'test' partition which also contains 7405 pairs.\n",
        "\n",
        "The HotpotQA XAI tool functions by using natural, multi-hop questions, with strong supervision for supporting facts via Wikipedia-based content to enable more explainable question answering systems. With the Gemma PX model, the HotpotQA XAI tool is applied using the first 15 questions from the validation partition of the 'fullwiki' dataset. The generated answer from the Gemma PX model is compared with the ground truth answer contained within the dataset, and an F1 score is calculated from comparing the generated answer with the ground truth answer with each question."
      ],
      "metadata": {
        "id": "g_0v7jfQYllq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the HotpotQA library on the Gemma PX model. Source: https://hotpotqa.github.io/?ref=the-batch-deeplearning-ai\n",
        "# Load the Wikipedia-based HotpotQA library using the 'fullwiki' option\n",
        "hpQADataset = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
        "\n",
        "# Use the 'validation' partition of the 'fullwiki' dataset and place it within its own variable\n",
        "validationDataset = hpQADataset['validation']\n",
        "\n",
        "# Select the first 15 questions within the validation samples dataset, to use HotpotQA techniques with the Gemma PX model\n",
        "validationSamples = validationDataset.select(range(15))\n",
        "\n",
        "# For each question in the list of 15 selected validation question, print them out for display and inspection purposes\n",
        "for vs in validationSamples:\n",
        "  print(vs)\n",
        "\n",
        "# Define a function which tokenizes inputs and context strings from each validation sample, for the Gemma PX model to process\n",
        "def tokenizeInput(question, context):\n",
        "  contextstr = \" \".join(context)\n",
        "  modelInputs = tokenizer.encode_plus(question, contextstr, return_tensors=\"pt\")\n",
        "  return modelInputs\n",
        "\n",
        "# Define a function which computes an F1 for each answer the Gemma PX model, it compares an output prediction with a truthful prediction and calculates precision/recall, which can then be used to calculate an F1 score\n",
        "def computeF1(pred, true):\n",
        "  predTokens = pred.lower().split()\n",
        "  trueTokens = true.lower().split()\n",
        "\n",
        "  commonTokens = set(predTokens) & set(trueTokens)\n",
        "  if len(commonTokens) == 0:\n",
        "    return 0.0\n",
        "\n",
        "  precision = len(commonTokens) / len(predTokens)\n",
        "  recall = len(commonTokens) / len(trueTokens)\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "# For each question in the validation dataset, separate each element into their own variable and input the question into the Gemma PX model. Compare the output prediction of the Gemma PX model with the truthful prediction and output/print performance metrics.\n",
        "for i, sample in enumerate(validationSamples):\n",
        "  question = sample['question']\n",
        "  context = sample['context']\n",
        "  groundTruth = sample.get('answer')\n",
        "\n",
        "  if groundTruth is None:\n",
        "    print(f\"Sample {i+1} has no ground truth text. Skipping.\")\n",
        "    continue\n",
        "\n",
        "  tokenizedInput = tokenizeInput(question, context)\n",
        "  inputids = tokenizedInput[\"input_ids\"]\n",
        "  modelOutputs = model.generate(inputids, max_length=150)\n",
        "  modelAnswer = tokenizer.decode(modelOutputs[0], skip_special_tokens=True)\n",
        "\n",
        "  exactMatch = modelAnswer.strip().lower() == groundTruth.strip().lower()\n",
        "  F1Score = computeF1(modelAnswer, groundTruth)\n",
        "\n",
        "  print(f\"Sample {i+1}\")\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Generated Answer: {modelAnswer}\")\n",
        "  print(f\"Ground Truth: {groundTruth}\")\n",
        "  print(f\"Exact Match: {exactMatch}\")\n",
        "  print(f\"F1 Score: {F1Score}\")\n",
        "  print(\"----------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "yWbp0EtVZNyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HotpotQA Explanations for the Gemma IT PX Model"
      ],
      "metadata": {
        "id": "t01M-IuRzfu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The HotpotQA XAI tool is a question-answering dataset, created by a collective team of NLP researchers at Carnegie Mellon University, Stanford University, and Université de Montréal. The HotpotQA XAI tool question-answering dataset is Wikipedia-based, with 113k question-answer pairs making up its entirety. These question-answer pairs are split up into various partitions, a 'train' partition which contains 90447 pairs, a 'validation' partition which contains 7405 pairs and a 'test' partition which also contains 7405 pairs.\n",
        "\n",
        "The HotpotQA XAI tool functions by using natural, multi-hop questions, with strong supervision for supporting facts via Wikipedia-based content to enable more explainable question answering systems. With the Gemma IT PX model, the HotpotQA XAI tool is applied using the first 15 questions from the validation partition of the 'fullwiki' dataset. The generated answer from the Gemma IT PX model is compared with the ground truth answer contained within the dataset, and an F1 score is calculated from comparing the generated answer with the ground truth answer with each question."
      ],
      "metadata": {
        "id": "ne6jkTbu2Kcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the HotpotQA library on the Gemma IT PX model. Source: https://hotpotqa.github.io/?ref=the-batch-deeplearning-ai\n",
        "# Load the Wikipedia-based HotpotQA library using the 'fullwiki' option\n",
        "hpQADataset2 = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
        "\n",
        "# Use the 'validation' partition of the 'fullwiki' dataset and place it within its own variable\n",
        "validationDataset2 = hpQADataset2['validation']\n",
        "\n",
        "# Select the first 15 questions within the validation samples dataset, to use HotpotQA techniques with the Gemma IT PX model\n",
        "validationSamples2 = validationDataset2.select(range(15))\n",
        "\n",
        "# For each question in the list of 15 selected validation question, print them out for display and inspection purposes\n",
        "for vs in validationSamples2:\n",
        "  print(vs)\n",
        "\n",
        "# Define a function which tokenizes inputs and context strings from each validation sample, for the Gemma IT PX instruction-tuned model to process\n",
        "def tokenizeInput(question, context):\n",
        "  contextstr = \" \".join(context)\n",
        "  modelInputs = tokenizer2.encode_plus(question, contextstr, return_tensors=\"pt\")\n",
        "  return modelInputs\n",
        "\n",
        "# Define a function which computes an F1 for each answer the Google Gemma 2B instruction-tuned model, it compares an output prediction with a truthful prediction and calculates precision/recall, which can then be used to calculate an F1 score\n",
        "def computeF1(pred, true):\n",
        "  predTokens = pred.lower().split()\n",
        "  trueTokens = true.lower().split()\n",
        "\n",
        "  commonTokens = set(predTokens) & set(trueTokens)\n",
        "  if len(commonTokens) == 0:\n",
        "    return 0.0\n",
        "\n",
        "  precision = len(commonTokens) / len(predTokens)\n",
        "  recall = len(commonTokens) / len(trueTokens)\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "# For each question in the validation dataset, separate each element into their own variable and input the question into the Gemma 2B instruction-tuned model. Compare the output prediction of the Google Gemma 2B model with the truthful prediction and output/print performance metrics.\n",
        "for i, sample in enumerate(validationSamples2):\n",
        "  question = sample['question']\n",
        "  context = sample['context']\n",
        "  groundTruth = sample.get('answer')\n",
        "\n",
        "  if groundTruth is None:\n",
        "    print(f\"Sample {i+1} has no ground truth text. Skipping.\")\n",
        "    continue\n",
        "\n",
        "  tokenizedInput2 = tokenizeInput(question, context)\n",
        "  inputids2 = tokenizedInput2[\"input_ids\"]\n",
        "  modelOutputs2 = model2.generate(inputids2, max_length=150)\n",
        "  modelAnswer2 = tokenizer2.decode(modelOutputs2[0], skip_special_tokens=True)\n",
        "\n",
        "  exactMatch = modelAnswer2.strip().lower() == groundTruth.strip().lower()\n",
        "  F1Score = computeF1(modelAnswer2, groundTruth)\n",
        "\n",
        "  print(f\"Sample {i+1}\")\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Generated Answer: {modelAnswer2}\")\n",
        "  print(f\"Ground Truth: {groundTruth}\")\n",
        "  print(f\"Exact Match: {exactMatch}\")\n",
        "  print(f\"F1 Score: {F1Score}\")\n",
        "  print(\"----------------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "tHxuVbKM0BaS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_nJ166H5n2MY",
        "6bLb_-iXefnD",
        "5Knxwt1fiFvx",
        "TNDvQJURaT6k",
        "cXy3nl-ZZ4-F",
        "7tuUBpQdq1og",
        "coisJ2sc60g5",
        "vA0QEW_Nsv_Q",
        "YctSJr_KwIms",
        "jfA9SNsL2bQ_",
        "n-Y7dtEW3ntx",
        "AK_ifufnFHkt",
        "n--iL8NoUhJH",
        "DKWsX--qLlcZ",
        "eePBueab6E6e",
        "htiifTcpC6Wk",
        "vJyrH9P-E_Fw",
        "uGLKA-ClJmxG",
        "Tc2Vm0WJFQn_",
        "eJYxgSwcEyLc",
        "7FMPUdCRhKoD",
        "EENCxtpr72cE",
        "cyxyYVzR-AvV",
        "s53_tAtdDuku",
        "lClmrRtkYtPb",
        "C_g9aBpia9P8",
        "ntyTs6GxQA7c",
        "MDRrcxoRy9_Y",
        "n0ubMkHq5eWH",
        "d6RDOX77AnwI",
        "OD-t5uYaB3xQ",
        "BuDK8RyLP3xu",
        "gMX533WcQ9UH",
        "iLsc3y7vbADQ",
        "a0PPzX68bf0w",
        "x7ATVXsFlcs4",
        "Jl4XFIilpNN1",
        "agd6jX0iKNa-",
        "11A8acvXUoso",
        "5ZGpJXSqhElA",
        "Ve_tfz8DfkxU",
        "pfKjyBbvfnCw",
        "Xqyx1f0iYTgT",
        "qCoWAiVmdrdp",
        "wpVC97rwmsIV",
        "mZLt0X9Gmtgj",
        "rLeqTE68y9yA",
        "1yVRGGxKUX4A",
        "wytGTLyIUcPl"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}